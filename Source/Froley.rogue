module Froley
  uses ParseKit<<Froley>>

$include "Assembler.rogue"
$include "Cmd.rogue"
$include "CmdTypeInfo.rogue"
$include "FroleyError.rogue"
$include "FroleyParser.rogue"
$include "FroleyTokenizer.rogue"
$include "Int32DefsBuilder.rogue"
$include "Label.rogue"
$include "ParserAssembler.rogue"
$include "ParserMethod.rogue"
$include "ParserOpcode.rogue"
$include "ScanState.rogue"
$include "Token.rogue"
$include "TokenType.rogue"
$include "TokenizerAssembler.rogue"
$include "TokenizerOpcode.rogue"
$include "TokenizerVM.rogue"

$includeFolder "CodeGenerators"
$includeFolder "CodeGenerators/Template"

Launcher( System.command_line_arguments )

class Launcher
  METHODS
    method init( args:String[] )
      try
        local options = String[]
        local inputs  = String[]
        forEach (arg in args)
          if (arg.begins_with("--")) options.add( arg.after_first("--") )
          else                       inputs.add( arg )
        endForEach

        which (inputs.count)
          case 0
            print_usage
            return

          case 1
            if (not inputs.first.ends_with(".froley",&ignore_case)) throw FroleyError( 'Input file must end with ".froley".' )

          others
            forEach (input in inputs)
              if (not input.ends_with(".froley",&ignore_case))
                throw FroleyError( "Invalid option or input file '$'." (input) )
              endIf
            endForEach
            throw FroleyError( "Only one .froley input file can be specified." )
        endWhich

        init( inputs.first, options )

      catch (error:Error)
        if (error not instanceOf FroleyError) error = FroleyError( error.message )
        # Turn other errors into FroleyErrors to get the nice banner-style output

        Console.error.println error
        System.exit 1

      endTry

    method init( input_filepath:String, options:String[] )
      try
        if (not File.exists(input_filepath))
          throw FroleyError( "Cannot read input file $." (input_filepath) )
        endIf

        local active_generators = CodeGenerator[]
        local cur_gen : CodeGenerator

        local language = input_filepath.before_last( ".froley", &ignore_case )

        forEach (option in options)
          local name = option.before_first('=')
          local value = option.after_first('=')
          if (name == "language")
            language = value
          else
            local gen = Froley.code_generators[ name ]
            if (gen)
              cur_gen = gen
              active_generators.add( cur_gen )
            elseIf (cur_gen is null)
              throw FroleyError( "Invalid option '$'. Note: no output language has been specified."(option) )
            else
              cur_gen.add_option( name, value )
            endIf
          endIf
        endForEach

        local info = Froley.parse( File(input_filepath) )
        forEach (gen in active_generators)
          info//language = language
          gen.process( info )
        endForEach

      catch (error:Error)
        if (error not instanceOf FroleyError) error = FroleyError( error.message )
        # Turn other errors into FroleyErrors to get the nice banner-style output

        Console.error.println error
        System.exit 1

      endTry

    method print_usage( writer=Console:PrintWriter )->PrintWriter
      writer.println @|USAGE
               |  froley filename.froley [host-language] [options]
               |
               |HOST LANGUAGES
               |  Froley can generate code for the following host languages:
      forEach (key in Froley.code_generators.keys)
        writer.print  "    --"
        writer.println key
      endForEach
      writer.println
      writer.println @|Use '--<host-language>' to include output for that language. Any successive
                      |options will apply to that language only. Multiple host languages can be
                      |specified.
                      |
                      |GENERAL OPTIONS
                      |  --language=<name>
                      |    Specifies the name of the language that the Froley-generated Tokenizer and
                      |    Parser will parse. For example, to generate Rogue source code that will
                      |    parse a language called Simple you would write:
                      |      froley --language=Simple --rogue
                      |    If no --language is specified then the base name of the '.froley' filename
                      |    is used as the language name.
                      |
                      |OPTIONS
      forEach (key in Froley.code_generators.keys)
        writer.print  "  --"
        writer.println key
        writer.println(forEach in Froley.code_generators[ key ].usage_options.sort((a,b)=>a<b)).println
      endForEach

      writer.println
      return writer
endClass

class TokenDef( name:String, type:Int32, symbol=null:String, attributes=0:Int32 )
  METHODS
    method to->String
      return "$=$ ($) [$]" (name,type,symbol,attributes)

    method to->Value
      return @{ :name, :type, :attributes, symbol:select{symbol||name} }
endClass

class Froley [singleton]
  PROPERTIES
    code_generators       = StringTable<<CodeGenerator>>()

    token_defs_by_name    = LookupList<<TokenDef>>()
    token_defs_by_type    = Table<<Int32,TokenDef>>()
    token_defs_by_symbol  = LookupList<<TokenDef>>()
    last_token_type       = 0

    token_defs_by_section = StringTable<<TokenDef[]>>()
    token_attributes      = LookupList<<Int32>>()

    entry_points          = StringLookupList()

    tokenizer_code = StringBuilder( 2048 )
    parser_code    = StringBuilder( 2048 )

    parser_methods = LookupList<<ParserMethod>>()
    id_tags        = StringTable<<Token>>()

  METHODS
    method parse( file:File )->Value
      local source = file.load_as_string
      return parse( file.filepath, source )

    method parse( filepath:String, source:String )->Value
      collect_definitions_and_extract_code( filepath, source )

      local tokenizer_statements = CmdStatements()
      local parser = FroleyParser()
      parser.set_source( filepath, tokenizer_code->String )
      parser.consume_eols
      parser.parse_multi_line_tokenizer_statements( tokenizer_statements )
      tokenizer_statements.resolve

      forEach (def in token_defs_by_name)
        if (def.symbol) token_defs_by_symbol[ def.symbol ] = def
        else            token_defs_by_symbol[ def.name ]   = def
      endForEach

      local tokenizer_bytes = TokenizerAssembler().assemble( tokenizer_statements )

      parser.set_source( filepath, parser_code->String )
      parser.parse_parser_methods
      local parser_assembler = ParserAssembler()
      local parser_bytes = parser_assembler.assemble( parser_methods )

      #{
      local max_count  = 0
      local max_digits = 0
      forEach (def in token_defs_by_name)
        max_count = max_count.or_larger( def.name.count )
        max_digits = max_digits.or_larger( def.type.digit_count )
      endForEach
      }#

      local token_attributes = @[]
      forEach (name at index in this.token_attributes.keys)
        local value = this.token_attributes[ index ]
        token_attributes.add( @{:name,:value} )
      endForEach

      local token_types = @[]
      forEach (def at index in token_defs_by_name)
        token_types.add( def->Value )
      endForEach

      local tokenizer_opcodes = @[]
      forEach (name in TokenizerOpcode.names)
        tokenizer_opcodes.add( @{ :name, value:TokenizerOpcode(name)->Int32 } )
      endForEach

      local cmd_types = @[]
      cmd_types.add( (forEach in parser_assembler.cmd_type_lookup)->Value )

      local parser_opcodes = @[]
      forEach (name in ParserOpcode.names)
        parser_opcodes.add( @{ :name, value:ParserOpcode(name)->Int32 } )
      endForEach

      #{
      local b64 = tokenizer_bytes.to_base64
      while (b64.count >= 64)
        println b64.leftmost(64)
        b64 = b64.rightmost(-64)
      endWhile
      if (b64.count) println b64
      }#
      #@trace tokenizer_bytes.count

      local output =
      @{
        language: "TODO",
        source_filepath: filepath,
        :token_attributes,
        :token_types,
        :tokenizer_opcodes,
        tokenizer_code:tokenizer_bytes.to_base64,
        :cmd_types,
        :parser_opcodes,
        parser_code:parser_bytes.to_base64
      }

      return output

    method collect_definitions_and_extract_code( filepath:String, source:String )
      $localDefine PARSING_CONFIGURE   0
      $localDefine PARSING_ATTRIBUTES  1
      $localDefine PARSING_DEFINITIONS 2
      $localDefine PARSING_TOKENIZER   3
      $localDefine PARSING_PARSER      4

      token_attributes.clear
      token_attributes[ "content" ] = 1

      tokenizer_code.clear
      parser_code.clear
      local cur_section_name = "tokenizer"
      local cur_section_defs : TokenDef[]
      local parse_type = PARSING_TOKENIZER
      local buffer = StringBuilder()
      local next_attribute_value = 1
      forEach (line at index in LineReader(source))
        if (line.begins_with('['))  # new section
          tokenizer_code.println  # keep line numbers consistent
          parser_code.println  # keep line numbers consistent
          cur_section_name = line.extract_string( "[$]*" )
          which (cur_section_name)
            case "configure"
              parse_type = PARSING_CONFIGURE
            case "attributes"
              parse_type = PARSING_ATTRIBUTES
            case "tokenizer"
              parse_type = PARSING_TOKENIZER
            case "parser"
              parse_type = PARSING_PARSER
            others
              parse_type = PARSING_DEFINITIONS
          endWhich

          if (parse_type == PARSING_DEFINITIONS)
            cur_section_defs = section_defs( cur_section_name )
          endIf

        elseIf (parse_type == PARSING_TOKENIZER)
          tokenizer_code.println( line )
          parser_code.println  # keep line numbers consistent

        elseIf (parse_type == PARSING_PARSER)
          parser_code.println( line )
          tokenizer_code.println  # keep line numbers consistent

        elseIf (parse_type == PARSING_ATTRIBUTES)
          tokenizer_code.println  # keep line numbers consistent
          parser_code.println  # keep line numbers consistent
          local scanner = Scanner( line, &spaces_per_tab=2 ).[ line=index+1 ]
          discard_whitespace( scanner )
          if (scanner.has_another)
            local name = scanner.scan_identifier
            if (name is null)
              throw FroleyError( filepath, source, scanner.line, scanner.column, "Attribute name expected." )
            endIf

            discard_whitespace( scanner )
            if (scanner.consume('='))
              discard_whitespace( scanner )
              if (not scanner.peek.is_number)
                throw FroleyError( filepath, source, scanner.line, scanner.column, "Attribute value expected, e.g 1, 2, 4, etc." )
              endIf
              next_attribute_value = scanner.scan_int64
            endIf

            token_attributes[ name ] = next_attribute_value
            next_attribute_value *= 2

          endIf
        else
          # Token definitions
          tokenizer_code.println  # keep line numbers consistent
          parser_code.println  # keep line numbers consistent
          local scanner = Scanner( line, &spaces_per_tab=2 ).[ line=index+1 ]
          discard_whitespace( scanner )
          if (scanner.has_another)
            local name = scanner.scan_identifier
            if (name is null)
              throw FroleyError( filepath, source, scanner.line, scanner.column, "Token name expected." )
            endIf

            local symbol : String
            discard_whitespace( scanner )
            if (scanner.has_another)
              buffer.clear
              local ch = scanner.peek
              if (ch == '"' or ch == '\'')
                local st = scanner.scan_string
                if (st is null)
                  throw FroleyError( filepath, source, scanner.line, scanner.column, "Unterminated string." )
                endIf
                buffer.print( st )
              else
                while (scanner.has_another and not scanner.consume(' ')) buffer.print( scanner.read )
              endIf
              symbol = buffer->String
            endIf

            local attributes = 0
            discard_whitespace( scanner )
            if (scanner.consume('['))
              discard_whitespace( scanner )
              local first = true
              while (first or scanner.consume(','))
                first = false
                discard_whitespace( scanner )
                local attribute_name = scanner.scan_identifier
                if (attribute_name is null)
                  throw FroleyError( filepath, source, scanner.line, scanner.column, "Attribute name expected." )
                endIf
                if (not token_attributes.contains(attribute_name))
                  throw FroleyError( filepath, source, scanner.line, scanner.column, "Undefined attribute '$'." (attribute_name) )
                endIf
                attributes |= token_attributes[ attribute_name ]
              endWhile
              if (not scanner.consume(']'))
                throw FroleyError( filepath, source, scanner.line, scanner.column, "Closing ']' expected." )
              endIf
            endIf

            discard_whitespace( scanner )
            if (scanner.has_another)
              throw FroleyError( filepath, source, scanner.line, scanner.column, "Syntax error - unexpected '$'." (scanner.peek.to_escaped_ascii) )
            endIf

            local def = token_def( name, cur_section_name )
            def.symbol = symbol
            def.attributes = attributes

          endIf
        endIf
      endForEach

    method discard_whitespace( scanner:Scanner )
      while (scanner.consume(' ')) noAction
      if (scanner.consume('#'))
        while (scanner.has_another) scanner.read
      endIf

    method section_defs( name:String )->TokenDef[]
      local entry = token_defs_by_section.find( name )
      if (entry) return entry.value

      local defs = TokenDef[]
      token_defs_by_section[ name ] = defs
      return defs

    method token_def( token_name:String, cur_section_name=null:String )->TokenDef
      if (token_defs_by_name.contains(token_name)) return token_defs_by_name[ token_name ]

      ++last_token_type
      local def = TokenDef( token_name, last_token_type )
      token_defs_by_name[ token_name ] = def
      token_defs_by_type[ def.type ] = def

      if (cur_section_name)
        section_defs( cur_section_name ).add( def )
      endIf
      return def

    method token_def( token_type:Int32 )->TokenDef
      return token_defs_by_type[ token_type ]

endClass

class TestVM : TokenizerVM
  METHODS
    method accept( token_type:Int32 )
      @trace token_type, buffer
endClass

