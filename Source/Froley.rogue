module Froley
  uses ParseKit<<Froley>>

$include "Assembler.rogue"
$include "Cmd.rogue"
$include "FroleyError.rogue"
$include "FroleyParser.rogue"
$include "FroleyTokenizer.rogue"
$include "Int32DefsBuilder.rogue"
$include "Label.rogue"
$include "ParserAssembler.rogue"
$include "ParserMethod.rogue"
$include "ParserOpcode.rogue"
$include "ScanState.rogue"
$include "Token.rogue"
$include "TokenType.rogue"
$include "TokenizerAssembler.rogue"
$include "TokenizerOpcode.rogue"
$include "TokenizerVM.rogue"

$include "CodeGenerators"

Launcher( System.command_line_arguments )

class Launcher
  METHODS
    method init( args:String[] )
      local options = String[]
      local targets = String[]
      forEach (arg in args)
        if (arg.begins_with("--")) options.add( arg.after_first("--") )
        else                       targets.add( arg )
      endForEach

      local first_ends_with = targets.count and targets.first.ends_with( ".froley", &ignore_case )
      local last_ends_with = targets.count == 2 and targets.last.ends_with( ".froley", &ignore_case )

      which (targets.count)
        case 0
          print_usage
          return

        case 1
          if (not first_ends_with) throw FroleyError( 'Input file must end with ".froley".' )
          targets.add( "json" )

        case 2
          if (first_ends_with)
            if (last_ends_with) throw FroleyError( "Froley can only compile one .froley file at a time." )
          elseIf (last_ends_with)
            targets.add( targets.remove_first )  # swap first and last
          else
            throw FroleyError( "No .froley input file specified." )
          endIf
      endWhich

      init( options, targets.first, targets.last )

    method init( options:String[], input_filepath:String, host_language:String  )
      try
        if (not File.exists(input_filepath))
          throw FroleyError( "Cannot read input file $." (input_filepath) )
        endIf

        if (not Froley.code_generators.contains(host_language))
          throw FroleyError( "Host language '$' is not supported." (host_language) )
        endIf

        local gen = Froley.code_generators[ host_language ]
        forEach (option in options)
          gen.add_option( option.before_first('='), option.after_first('=') )
        endForEach

        local info = Froley.parse( File(input_filepath) )
        local language = select{ gen.options//language || input_filepath.before_last(".froley",&ignore_case) }
        info//language = language

        gen.process( info )

      catch (error:Error)
        if (error not instanceOf FroleyError) error = FroleyError( error.message )
        # Turn other errors into FroleyErrors to get the nice banner-style output

        Console.error.println error
        System.exit 1

      endTry

    method print_usage
      println @|USAGE
               |  froley filename.froley [host-language] [options]
               |
               |HOST LANGUAGES
               |  Froley can generate code for the following host languages:
      forEach (key in Froley.code_generators.keys)
        print  "    - "
        println key
      endForEach
      println

      println @|OPTIONS
      forEach (key in Froley.code_generators.keys)
        print  "  "
        println key
        println(forEach in Froley.code_generators[ key ].usage_options.sort((a,b)=>a<b)).println
        println
      endForEach

      println
endClass

class TokenDef( name:String, type:Int32, symbol=null:String, attributes=0:Int32 )
  METHODS
    method to->String
      return "$=$ ($) [$]" (name,type,symbol,attributes)
endClass

class Froley [singleton]
  PROPERTIES
    code_generators       = StringTable<<CodeGenerator>>()

    token_defs_by_name    = LookupList<<TokenDef>>()
    token_defs_by_type    = Table<<Int32,TokenDef>>()
    token_defs_by_symbol  = LookupList<<TokenDef>>()
    last_token_type       = 0

    token_defs_by_section = StringTable<<TokenDef[]>>()
    token_attributes      = LookupList<<Int32>>()

    entry_points          = StringLookupList()

    tokenizer_code = StringBuilder( 2048 )
    parser_code    = StringBuilder( 2048 )

    parser_methods = LookupList<<ParserMethod>>()

  METHODS
    method parse( file:File )->Value
      local source = file.load_as_string
      return parse( file.filepath, source )

    method parse( filepath:String, source:String )->Value
      collect_definitions_and_extract_code( filepath, source )

      local tokenizer_statements = CmdStatements()
      local parser = FroleyParser()
      parser.set_source( filepath, tokenizer_code->String )
      parser.consume_eols
      parser.parse_multi_line_tokenizer_statements( tokenizer_statements )
      tokenizer_statements.resolve

      forEach (def in token_defs_by_name)
        if (def.symbol) token_defs_by_symbol[ def.symbol ] = def
        else            token_defs_by_symbol[ def.name ]   = def
      endForEach

      local tokenizer_bytes = TokenizerAssembler().assemble( tokenizer_statements )

      parser.set_source( filepath, parser_code->String )
      parser.parse_parser_methods
      local parser_assembler = ParserAssembler()
      local parser_bytes = parser_assembler.assemble( parser_methods )

      #{
      local max_count  = 0
      local max_digits = 0
      forEach (def in token_defs_by_name)
        max_count = max_count.or_larger( def.name.count )
        max_digits = max_digits.or_larger( def.type.digit_count )
      endForEach
      }#

      local token_types = @[]
      forEach (def at index in token_defs_by_name)
        local symbol = select{ def.symbol || def.name }
        token_types.add( @{ name:def.name, type:def.type, symbol:symbol } )
      endForEach

      local tokenizer_opcodes = @[]
      forEach (name in TokenizerOpcode.names)
        tokenizer_opcodes.add( @{ :name, value:TokenizerOpcode(name)->Int32 } )
      endForEach

      local parser_cmd_type_names = @[]
      parser_cmd_type_names.add( forEach in parser_assembler.cmd_type_names )

      local parser_opcodes = @[]
      forEach (name in ParserOpcode.names)
        parser_opcodes.add( @{ :name, value:ParserOpcode(name)->Int32 } )
      endForEach

      #{
      local b64 = tokenizer_bytes.to_base64
      while (b64.count >= 64)
        println b64.leftmost(64)
        b64 = b64.rightmost(-64)
      endWhile
      if (b64.count) println b64
      }#
      #@trace tokenizer_bytes.count

      local output =
      @{
        language: "TODO",
        source_filepath: filepath,
        :token_types,
        :tokenizer_opcodes,
        tokenizer_code:tokenizer_bytes.to_base64,
        :parser_cmd_type_names,
        :parser_opcodes,
        parser_code:parser_bytes.to_base64
      }

      return output

    method collect_definitions_and_extract_code( filepath:String, source:String )
      $localDefine PARSING_CONFIGURE   0
      $localDefine PARSING_ATTRIBUTES  1
      $localDefine PARSING_DEFINITIONS 2
      $localDefine PARSING_TOKENIZER   3
      $localDefine PARSING_PARSER      4
      tokenizer_code.clear
      parser_code.clear
      local cur_section_name = "tokenizer"
      local cur_section_defs : TokenDef[]
      local parse_type = PARSING_TOKENIZER
      local buffer = StringBuilder()
      local next_attribute_value = 1
      forEach (line at index in LineReader(source))
        if (line.begins_with('['))  # new section
          tokenizer_code.println  # keep line numbers consistent
          parser_code.println  # keep line numbers consistent
          cur_section_name = line.extract_string( "[$]*" )
          which (cur_section_name)
            case "configure"
              parse_type = PARSING_CONFIGURE
            case "attributes"
              parse_type = PARSING_ATTRIBUTES
            case "tokenizer"
              parse_type = PARSING_TOKENIZER
            case "parser"
              parse_type = PARSING_PARSER
            others
              parse_type = PARSING_DEFINITIONS
          endWhich

          if (parse_type == PARSING_DEFINITIONS)
            cur_section_defs = section_defs( cur_section_name )
          endIf

        elseIf (parse_type == PARSING_TOKENIZER)
          tokenizer_code.println( line )
          parser_code.println  # keep line numbers consistent

        elseIf (parse_type == PARSING_PARSER)
          parser_code.println( line )
          tokenizer_code.println  # keep line numbers consistent

        elseIf (parse_type == PARSING_ATTRIBUTES)
          tokenizer_code.println  # keep line numbers consistent
          parser_code.println  # keep line numbers consistent
          local scanner = Scanner( line, &spaces_per_tab=2 ).[ line=index+1 ]
          discard_whitespace( scanner )
          if (scanner.has_another)
            local name = scanner.scan_identifier
            if (name is null)
              throw FroleyError( filepath, source, scanner.line, scanner.column, "Attribute name expected." )
            endIf

            discard_whitespace( scanner )
            if (scanner.consume('='))
              discard_whitespace( scanner )
              if (not scanner.peek.is_number)
                throw FroleyError( filepath, source, scanner.line, scanner.column, "Attribute value expected, e.g 1, 2, 4, etc." )
              endIf
              next_attribute_value = scanner.scan_int64
            endIf

            token_attributes[ name ] = next_attribute_value
            next_attribute_value *= 2

          endIf
        else
          # Token definitions
          tokenizer_code.println  # keep line numbers consistent
          parser_code.println  # keep line numbers consistent
          local scanner = Scanner( line, &spaces_per_tab=2 ).[ line=index+1 ]
          discard_whitespace( scanner )
          if (scanner.has_another)
            local name = scanner.scan_identifier
            if (name is null)
              throw FroleyError( filepath, source, scanner.line, scanner.column, "Token name expected." )
            endIf

            local symbol : String
            discard_whitespace( scanner )
            if (scanner.has_another)
              buffer.clear
              local ch = scanner.peek
              if (ch == '"' or ch == '\'')
                local st = scanner.scan_string
                if (st is null)
                  throw FroleyError( filepath, source, scanner.line, scanner.column, "Unterminated string." )
                endIf
                buffer.print( st )
              else
                while (scanner.has_another and not scanner.consume(' ')) buffer.print( scanner.read )
              endIf
              symbol = buffer->String
            endIf

            local attributes = 0
            discard_whitespace( scanner )
            if (scanner.consume('['))
              discard_whitespace( scanner )
              local first = true
              while (first or scanner.consume(','))
                first = false
                discard_whitespace( scanner )
                local attribute_name = scanner.scan_identifier
                if (attribute_name is null)
                  throw FroleyError( filepath, source, scanner.line, scanner.column, "Attribute name expected." )
                endIf
                if (not token_attributes.contains(attribute_name))
                  throw FroleyError( filepath, source, scanner.line, scanner.column, "Undefined attribute '$'." (attribute_name) )
                endIf
                attributes |= token_attributes[ attribute_name ]
              endWhile
              if (not scanner.consume(']'))
                throw FroleyError( filepath, source, scanner.line, scanner.column, "Closing ']' expected." )
              endIf
            endIf

            discard_whitespace( scanner )
            if (scanner.has_another)
              throw FroleyError( filepath, source, scanner.line, scanner.column, "Syntax error - unexpected '$'." (scanner.peek.to_escaped_ascii) )
            endIf

            local def = token_def( name, cur_section_name )
            def.symbol = symbol
            def.attributes = attributes

          endIf
        endIf
      endForEach

    method discard_whitespace( scanner:Scanner )
      while (scanner.consume(' ')) noAction
      if (scanner.consume('#'))
        while (scanner.has_another) scanner.read
      endIf

    method section_defs( name:String )->TokenDef[]
      local entry = token_defs_by_section.find( name )
      if (entry) return entry.value

      local defs = TokenDef[]
      token_defs_by_section[ name ] = defs
      return defs

    method token_def( token_name:String, cur_section_name=null:String )->TokenDef
      if (token_defs_by_name.contains(token_name)) return token_defs_by_name[ token_name ]

      ++last_token_type
      local def = TokenDef( token_name, last_token_type )
      token_defs_by_name[ token_name ] = def
      token_defs_by_type[ def.type ] = def

      if (cur_section_name)
        section_defs( cur_section_name ).add( def )
      endIf
      return def

    method token_def( token_type:Int32 )->TokenDef
      return token_defs_by_type[ token_type ]

endClass

class TestVM : TokenizerVM
  METHODS
    method accept( token_type:Int32 )
      @trace token_type, buffer
endClass

