module Froley
  uses ParseKit<<Froley>>

  # TODO: if, return values, call args

class FroleyParser : Parser
  PROPERTIES
    parse_tokenizer_expression : ParseRule
    parse_tokenizer_term       : ParseRule
    parse_parser_expression    : ParseRule
    parse_rule_type            = ParseRuleType.GENERAL : ParseRuleType

  METHODS
    method init
      # tokenizer expression
      local rule = add( ParseRule("tokenizer_expression") )

      # tokenizer assignment
      rule = add_nested( RightAssociativeBinaryParseRule("assignment") )
      rule.on( "=", <<TokenizerCmdAssign>> )

      # tokenizer logical or
      rule = add_nested( BinaryParseRule("logical_or") )
      rule.on( "or", <<TokenizerCmdLogicalOr>> )

      # tokenizer logical and
      rule = add_nested( BinaryParseRule("logical_and") )
      rule.on( "and", <<TokenizerCmdLogicalAnd>> )

      # tokenizer comparison
      rule = add_nested( BinaryParseRule("comparison") )
      rule.on( "==", <<TokenizerCmdCompareEQ>> )
      rule.on( "!=", <<TokenizerCmdCompareNE>> )
      rule.on( "<",  <<TokenizerCmdCompareLT>> )
      rule.on( "<=", <<TokenizerCmdCompareLE>> )
      rule.on( ">",  <<TokenizerCmdCompareGT>> )
      rule.on( ">=", <<TokenizerCmdCompareGE>> )

      rule = add_nested( PreUnaryParseRule("pre_unary") )
      rule.on( "not", <<TokenizerCmdNot>> )

      # tokenizer is digit, is letter
      rule = add_nested(
        PostUnaryParseRule( "is_digit_is_letter",
          function (rule:ParseRule)->Cmd
            local t = rule.parser.peek
            local term = rule.parser.parse_tokenizer_term()
            local parser = rule.parser
            if (parser.consume(TokenType.KEYWORD_IS))
              if (parser.consume(TokenType.KEYWORD_NOT))
                if (parser.consume(TokenType.KEYWORD_DIGIT))
                  local base = 10
                  if (parser.consume(TokenType.KEYWORD_BASE))
                    if (parser.consume(TokenType.KEYWORD_COUNT))
                      return TokenizerCmdNot( t, term.to_is_digit_count )
                    elseIf (not parser.next_is(TokenType.INTEGER))
                      throw parser.peek.error( "Base '2'..'16' or 'count' expected." )
                    endIf
                    base = parser.read->Int32
                  endIf
                  return TokenizerCmdNot( t, term.to_is_digit(base) )
                elseIf (parser.consume(TokenType.KEYWORD_LETTER))
                  return TokenizerCmdNot( t, term.to_is_letter )
                else
                  throw parser.peek.error( "'is not digit' or 'is not letter' expected." )
                endIf
              else
                if (parser.consume(TokenType.KEYWORD_DIGIT))
                  local base = 10
                  if (parser.consume(TokenType.KEYWORD_BASE))
                    if (parser.consume(TokenType.KEYWORD_COUNT))
                      return term.to_is_digit_count
                    elseIf (not parser.next_is(TokenType.INTEGER))
                      throw parser.peek.error( "Base '2'..'16' or 'count' expected." )
                    endIf
                    base = parser.read->Int32
                  endIf
                  return term.to_is_digit( base )
                elseIf (parser.consume(TokenType.KEYWORD_LETTER))
                  return term.to_is_letter
                else
                  throw parser.peek.error( "'is digit' or 'is letter' expected." )
                endIf
              endIf
            else
              return term
            endIf
          endFunction
        )
      )

      # tokenizer term
      rule = add_nested( ParseRule("tokenizer_term") )
      rule.on( "(",
        function (parser:FroleyParser)->Cmd
          parser.must_consume( TokenType.SYMBOL_OPEN_PAREN )
          local result = parser.parse_tokenizer_expression()
          parser.must_consume( TokenType.SYMBOL_CLOSE_PAREN )
          return result
        endFunction
      )
      rule.on( "ch",            <<TokenizerCmdRegisterCh>> )
      rule.on( "count",         <<TokenizerCmdRegisterCount>> )
      rule.on( "hasAnother",    <<TokenizerCmdHasAnother>> )
      rule.on( "integer value", <<TokenizerCmdLiteralInt32>> )
      rule.on( "literal string",<<TokenizerCmdLiteralString>> )
      rule.on( "read",          <<TokenizerCmdRead>> )
      rule.on( "pop",           <<TokenizerCmdPop>> )
      rule.on( "identifier",
        function(parser:FroleyParser)->Cmd
          local t = parser.read  # (identifier)
          if (parser.consume(TokenType.SYMBOL_OPEN_PAREN))
            parser.must_consume( TokenType.SYMBOL_CLOSE_PAREN )
            return TokenizerCmdCall( t, t.text )
          else
            return TokenizerCmdAccess( t )
          endIf
        endFunction
      )
      rule.on( "scanDigits",
        function(parser:FroleyParser)->Cmd
          local t = parser.read  # 'scanDigits'
          local min_digits = parser.read_integer
          local max_digits = min_digits
          if (parser.consume(TokenType.SYMBOL_DOT_DOT)) max_digits = parser.read_integer
          local base = 10
          if (parser.consume(TokenType.KEYWORD_BASE))
            base = parser.read_integer
          endIf
          return TokenizerCmdScanDigits(t,min_digits,max_digits,base)
        endFunction
      )
      rule.on( "scanIdentifier", <<TokenizerCmdScanIdentfier>> )
      rule.on( "peek",
        function(parser:FroleyParser)->Cmd
          local t = parser.read  # 'peek'
          if (parser.next_is(TokenType.SYMBOL_OPEN_PAREN))
            local expr = parser.parse_tokenizer_expression()
            if (expr.is_integer) return TokenizerCmdPeekInt32( t, expr->Int32 )
            if (expr.is_count)   return TokenizerCmdPeekCount( t )
            throw expr.t.error( "'Expected 'peek', 'peek(<integer>)', or 'peek(count)'." )
          else
            return TokenizerCmdPeek( t )
          endIf
        endFunction
      )
      rule.on( "consume",
        function(parser:FroleyParser)->Cmd
          local t = parser.read  # 'consume'
          if (parser.next_is(TokenType.SYMBOL_OPEN_PAREN))
            local expr = parser.parse_tokenizer_expression()
            t = expr.t
            if (expr.is_integer) return TokenizerCmdConsumeCharacter( t, expr->Int32 )
          endIf
          throw t.error( "'Expected 'consume(<character>)'." )
        endFunction
      )

      # parser expression
      rule = add( ParseRule("parser_expression") )

      #{
      # parser assignment
      rule = add_nested( RightAssociativeBinaryParseRule("assignment") )
      rule.on( "=", <<TokenizerCmdAssign>> )
      }#

      # parser logical or
      rule = add_nested( BinaryParseRule("parser_logical_or") )
      rule.on( "or", <<ParserCmdLogicalOr>> )

      # parser logical and
      rule = add_nested( BinaryParseRule("parser_logical_and") )
      rule.on( "and", <<ParserCmdLogicalAnd>> )

      # parser comparison
      #rule = add_nested( BinaryParseRule("parser_comparison") )
      #rule.on( "==", <<TokenizerCmdCompareEQ>> )
      #rule.on( "!=", <<TokenizerCmdCompareNE>> )
      #rule.on( "<",  <<TokenizerCmdCompareLT>> )
      #rule.on( "<=", <<TokenizerCmdCompareLE>> )
      #rule.on( ">",  <<TokenizerCmdCompareGT>> )
      #rule.on( ">=", <<TokenizerCmdCompareGE>> )
      #rule.on( "is", <<ParserCmdTokenType>> )

      rule = add_nested( PreUnaryParseRule("parser_pre_unary") )
      rule.on( "not", <<ParserCmdLogicalNot>> )

      #{
      # parser is digit, is letter
      rule = add_nested(
        PostUnaryParseRule( "is_digit_is_letter",
          function (rule:ParseRule)->Cmd
            local t = rule.parser.peek
            local term = rule.parser.parse_tokenizer_term()
            local parser = rule.parser
            if (parser.consume(TokenType.KEYWORD_IS))
              if (parser.consume(TokenType.KEYWORD_NOT))
                if (parser.consume(TokenType.KEYWORD_DIGIT))
                  local base = 10
                  if (parser.consume(TokenType.SYMBOL_OPEN_PAREN))
                    if (parser.consume(TokenType.KEYWORD_COUNT))
                      parser.must_consume( TokenType.SYMBOL_CLOSE_PAREN )
                      return TokenizerCmdNot( t, term.to_is_digit_count )
                    elseIf (not parser.next_is(TokenType.INTEGER))
                      throw parser.peek.error( "Base '2'..'16' or 'count' expected." )
                    endIf
                    base = parser.read->Int32
                    parser.must_consume( TokenType.SYMBOL_CLOSE_PAREN )
                  endIf
                  return TokenizerCmdNot( t, term.to_is_digit(base) )
                elseIf (parser.consume(TokenType.KEYWORD_LETTER))
                  return TokenizerCmdNot( t, term.to_is_letter )
                else
                  throw parser.peek.error( "'is not digit' or 'is not letter' expected." )
                endIf
              else
                if (parser.consume(TokenType.KEYWORD_DIGIT))
                  local base = 10
                  if (parser.consume(TokenType.SYMBOL_OPEN_PAREN))
                    if (parser.consume(TokenType.KEYWORD_COUNT))
                      parser.must_consume( TokenType.SYMBOL_CLOSE_PAREN )
                      return term.to_is_digit_count
                    elseIf (not parser.next_is(TokenType.INTEGER))
                      throw parser.peek.error( "Base '2'..'16' or 'count' expected." )
                    endIf
                    base = parser.read->Int32
                    parser.must_consume( TokenType.SYMBOL_CLOSE_PAREN )
                  endIf
                  return term.to_is_digit( base )
                elseIf (parser.consume(TokenType.KEYWORD_LETTER))
                  return term.to_is_letter
                else
                  throw parser.peek.error( "'is digit' or 'is letter' expected." )
                endIf
              endIf
            else
              return term
            endIf
          endFunction
        )
      )
      }#

      # parser term
      rule = add_nested( ParseRule("parser_term") )
      rule.on( "(",
        function (parser:FroleyParser)->Cmd
          parser.must_consume( TokenType.SYMBOL_OPEN_PAREN )
          local result = parser.parse_parser_expression()
          parser.must_consume( TokenType.SYMBOL_CLOSE_PAREN )
          return result
        endFunction
      )
      rule.on( "true",  <<ParserCmdLiteralTrue>> )
      rule.on( "false", <<ParserCmdLiteralFalse>> )
      rule.on( "nextHasAttribute",
        function (parser:FroleyParser)->Cmd
          local t = parser.read  # 'nextHasAttribute'
          local name : String
          if (parser.consume(TokenType.SYMBOL_OPEN_PAREN))
            name = parser.read_identifier
            parser.must_consume( TokenType.SYMBOL_CLOSE_PAREN )
          else
            name = parser.read_identifier
          endIf
          if (Froley.token_attributes.contains(name))
            return ParserCmdNextHasAttribute( t, name )
          else
            throw t.error( "No such token attribute '$' (attributes are be defined in an [attributes] section of the .froley file)." (name) )
          endIf
        endFunction
      )
      rule.on( "nextIs",
        function(parser:FroleyParser)->Cmd
          local t = parser.read  # 'nextIs'
          if (parser.consume(TokenType.SYMBOL_OPEN_PAREN))
            local result = ParserCmdNextIsTokenType( t, parser.read_token_type_def )
            parser.must_consume( TokenType.SYMBOL_CLOSE_PAREN )
            return result
          else
            return ParserCmdNextIsTokenType( t, parser.read_token_type_def )
          endIf
        endFunction
      )

      #{
      rule.on( "ch",            <<TokenizerCmdRegisterCh>> )
      rule.on( "count",         <<TokenizerCmdRegisterCount>> )
      }#
      rule.on( "hasAnother",    <<ParserCmdHasAnother>> )
      #{
      rule.on( "integer value", <<TokenizerCmdLiteralInt32>> )
      rule.on( "literal string",<<TokenizerCmdLiteralString>> )
      rule.on( "read",          <<TokenizerCmdRead>> )
      rule.on( "pop",           <<TokenizerCmdPop>> )
      }#
      rule.on( "identifier",
        function(parser:FroleyParser)->Cmd
          local t = parser.read  # (identifier)
          return ParserCmdAccess( t )
          #{
          if (parser.consume(TokenType.SYMBOL_OPEN_PAREN))
            parser.must_consume( TokenType.SYMBOL_CLOSE_PAREN )
            return TokenizerCmdCall( t, t.text )
          else
            return TokenizerCmdAccess( t )
          endIf
          }#
        endFunction
      )
      #{
      rule.on( "scanDigits",
        function(parser:FroleyParser)->Cmd
          local t = parser.read  # 'scanDigits'
          local min_digits = parser.read_integer
          local max_digits = min_digits
          if (parser.consume(TokenType.SYMBOL_DOT_DOT)) max_digits = parser.read_integer
          local base = 10
          if (parser.consume(TokenType.KEYWORD_BASE))
            base = parser.read_integer
          endIf
          return TokenizerCmdScanDigits(t,min_digits,max_digits,base)
        endFunction
      )
      rule.on( "scanIdentifier", <<TokenizerCmdScanIdentfier>> )
      rule.on( "peek",
        function(parser:FroleyParser)->Cmd
          local t = parser.read  # 'peek'
          if (parser.next_is(TokenType.SYMBOL_OPEN_PAREN))
            local expr = parser.parse_tokenizer_expression()
            if (expr.is_integer) return TokenizerCmdPeekInt32( t, expr->Int32 )
            if (expr.is_count)   return TokenizerCmdPeekCount( t )
            throw expr.t.error( "'Expected 'peek', 'peek(<integer>)', or 'peek(count)'." )
          else
            return TokenizerCmdPeek( t )
          endIf
        endFunction
      )
    }#
      rule.on( "consume",
        function(parser:FroleyParser)->Cmd
          local t = parser.read  # 'consume'
          if (parser.consume(TokenType.SYMBOL_OPEN_PAREN))
            local result = ParserCmdConsumeTokenType( t, parser.read_token_type_def )
            parser.must_consume( TokenType.SYMBOL_CLOSE_PAREN )
            return result
          else
            return ParserCmdConsumeTokenType( t, parser.read_token_type_def )
          endIf
        endFunction
      )

    method parse_tokenizer_statements( statements:CmdStatements )->Logical
      if (consume_eols)
        parse_multi_line_tokenizer_statements( statements )
        return true
      else
        parse_single_line_tokenizer_statements( statements )
        return false
      endIf

    method parse_multi_line_tokenizer_statements( statements:CmdStatements )
      consume_eols
      while (reader.has_another and not reader.peek.type.is_structural)
        parse_tokenizer_statement( statements, &allow_control_structures )
        while (consume_eols or consume(TokenType.SYMBOL_SEMICOLON)) noAction
      endWhile

    method parse_single_line_tokenizer_statements( statements:CmdStatements )
      while (reader.has_another and not reader.peek.type.is_structural)
        parse_tokenizer_statement( statements, &!allow_control_structures )
        if (not consume(TokenType.SYMBOL_SEMICOLON)) return
        while (consume(TokenType.SYMBOL_SEMICOLON)) noAction

        # Don't let a trailing ';' act as a next-line continuation.
        if (next_is(TokenType.EOL)) escapeWhile
      endWhile

      if (not consume(TokenType.EOL))
        if (not reader.peek.type.is_structural)
          must_consume( TokenType.EOL )  # force an error
        endIf
      endIf

    method parse_tokenizer_statement( statements:CmdStatements, &allow_control_structures )
      consume_eols
      if (not has_another) return
      local t = peek
      if (t.type.is_structural) return

      if (allow_control_structures)
        if (consume(TokenType.KEYWORD_IF))
          local cmd_if = TokenizerCmdIf( t, parse_tokenizer_expression() )
          if (parse_tokenizer_statements(cmd_if.statements))
            # Multi-line statements
            local outer_if = cmd_if
            while (next_is_tokenizer_end_if(&multi_line))
              consume_eols
              local t2 = read
              local inner_if = TokenizerCmdIf( t2, parse_tokenizer_expression() )
              parse_multi_line_tokenizer_statements( inner_if.statements )
              ensure outer_if.else_statements
              outer_if.else_statements.add( inner_if )
              outer_if = inner_if
            endWhile
            if (next_is_else(&multi_line))
              consume_eols
              if (next_is(TokenType.KEYWORD_ELSE) and reader.peek(1).type == TokenType.EOL)
                read
                ensure outer_if.else_statements
                parse_multi_line_tokenizer_statements( outer_if.else_statements )
              endIf
            endIf
            must_consume( TokenType.KEYWORD_END_IF )
          else
            # Single line statements
            local outer_if = cmd_if
            while (next_is_tokenizer_end_if(&single_line))
              consume_eols
              local t2 = read
              local inner_if = TokenizerCmdIf( t2, parse_tokenizer_expression() )
              parse_single_line_tokenizer_statements( inner_if.statements )
              ensure outer_if.else_statements
              outer_if.else_statements.add( inner_if )
              outer_if = inner_if
            endWhile
            if (next_is_else(&single_line))
              consume_eols
              if (next_is(TokenType.KEYWORD_ELSE) and reader.peek(1).type != TokenType.EOL)
                read
                ensure outer_if.else_statements
                parse_single_line_tokenizer_statements( outer_if.else_statements )
              endIf
            endIf
          endIf
          statements.add( cmd_if )
          return

        elseIf (consume(TokenType.KEYWORD_WHILE))
          local cmd_while = TokenizerCmdWhile( t, parse_tokenizer_expression() )
          if (parse_tokenizer_statements(cmd_while.statements))
            # Returns true if multi-line statements were parsed
            must_consume( TokenType.KEYWORD_END_WHILE )
          endIf
          statements.add( cmd_while )
          return

        endIf
      endIf

      if (consume(TokenType.KEYWORD_ACCEPT))
        local name = read_identifier
        statements.add( TokenizerCmdAcceptInt32(t,Froley.token_def(name).type) )
        return

      elseIf (consume(TokenType.KEYWORD_CLEAR))
        if (consume(TokenType.KEYWORD_BUFFER))
          statements.add( CmdStatement(t,"clear buffer",TokenizerOpcode.CLEAR_BUFFER) )
          return
        else
          throw peek.error( "Expected 'buffer'." )
        endIf

      elseIf (consume(TokenType.KEYWORD_COLLECT))
        if (next_is(TokenType.KEYWORD_CH))
          must_consume_register_ch
          statements.add( CmdStatement(t,"collect ch",TokenizerOpcode.COLLECT_CH) )
          return
        elseIf (next_is(TokenType.STRING))
          statements.add( TokenizerCmdCollectString(t,read.text) )
          return
        elseIf (next_is(TokenType.INTEGER))
          statements.add( TokenizerCmdCollectCharacter(t,read_integer) )
          return
        else
          throw t.error( "Syntax error - expected 'ch' or a literal string.")
        endIf
        return

      elseIf (consume(TokenType.KEYWORD_DISCARD))
        statements.add( CmdStatement(t,"restart",TokenizerOpcode.RESTART) )
        return

      elseIf (consume(TokenType.KEYWORD_ERROR))
        if (consume(TokenType.KEYWORD_BUFFER))
          statements.add( CmdStatement(t,"error buffer",TokenizerOpcode.ERROR) )
        elseIf (next_is(TokenType.STRING))
          statements.add( CmdStatement(t,"clear buffer",TokenizerOpcode.CLEAR_BUFFER) )
          statements.add( TokenizerCmdCollectString(t,read.text) )
          statements.add( CmdStatement(t,"error <message>",TokenizerOpcode.ERROR) )
        else
          throw peek.error( "Expected 'buffer'." )
        endIf
        return

      elseIf (consume(TokenType.KEYWORD_GOTO))
        statements.add( TokenizerCmdGoto(t,read_identifier) )
        return

      elseIf (consume(TokenType.KEYWORD_HALT))
        statements.add( CmdStatement(t,"halt",TokenizerOpcode.HALT) )
        return

      elseIf (consume(TokenType.KEYWORD_MARK_SRC_POS))
        statements.add( CmdStatement(t,"markSourcePosition",TokenizerOpcode.MARK_SOURCE_POS) )
        return

      elseIf (consume(TokenType.KEYWORD_MODE))
        local label_name = read_label_name
        statements.add( TokenizerCmdMode(t,label_name) )
        return

      elseIf (consume(TokenType.KEYWORD_PRINT))
        if (consume(TokenType.KEYWORD_BUFFER))
          statements.add( CmdStatement(t,"print buffer",TokenizerOpcode.PRINT_BUFFER) )
          return
        elseIf (next_is(TokenType.STRING))
          statements.add( TokenizerCmdPrintString(t,read.text) )
          return
        else
          if (not (next_is(TokenType.EOL) or next_is(TokenType.SYMBOL_SEMICOLON)))
            local expr = parse_tokenizer_expression()
            if (expr.is_ch)
              statements.add( CmdStatement(t,"print ch",TokenizerOpcode.PRINT_CH) )
              return
            elseIf (expr.is_count)
              statements.add( CmdStatement(t,"print count",TokenizerOpcode.PRINT_COUNT) )
              return
            elseIf (expr.is_integer)
              statements.add( TokenizerCmdPrintCharacter(t,expr->Int32) )
              return
            endIf
          endIf
          throw t.error( "Syntax error - valid 'print' arguments are  'ch', 'count', and '<string>'." )
        endIf

      elseIf (consume(TokenType.KEYWORD_RESTART))
        if (next_is(TokenType.IDENTIFIER))
          local t2 = peek
          statements.add( TokenizerCmdMode(t2,read_identifier) )
        endIf
        statements.add( CmdStatement(t,"restart",TokenizerOpcode.RESTART) )
        return

      elseIf (consume(TokenType.KEYWORD_RETURN))
        statements.add( CmdStatement(t,"return",TokenizerOpcode.RETURN) )
        return

      elseIf (consume(TokenType.KEYWORD_SCAN_DIGITS))
        throw t.error( "Syntax error. Expected 'ch = scanDigits <min-digits>[..<max-digits>] [base <number-base>]'." )

      elseIf (consume(TokenType.KEYWORD_WHICH))
        local has_parens = consume( TokenType.SYMBOL_OPEN_PAREN )
        local uses_input = consume( TokenType.KEYWORD_INPUT )
        if (not uses_input and not consume(TokenType.KEYWORD_BUFFER))
          throw t.error( "Expected 'which (input)' or 'which (buffer)'." )
        endIf
        if (has_parens) must_consume( TokenType.SYMBOL_CLOSE_PAREN )

        if (uses_input)
          local cmd_which_input = TokenizerCmdWhichInput( t )
          statements.add( cmd_which_input )
          parse_which_input( cmd_which_input )
        else
          # Uses buffer
          local cmd_which = TokenizerCmdWhichBuffer( t )
          statements.add( cmd_which )
          parse_which_buffer( cmd_which )
        endIf
        must_consume( TokenType.KEYWORD_END_WHICH )
        return

      elseIf (next_is(TokenType.SYMBOL_LT))
        local name = read_label_name
        must_consume( TokenType.EOL )
        statements.add( TokenizerCmdLabel(t,name) )
        return

      elseIf (consume(TokenType.SYMBOL_MINUS_MINUS))
        local target = parse_register
        statements.add( TokenizerCmdSubtract(t,target,TokenizerCmdLiteralInt32(t,1)) )
        return

      elseIf (consume(TokenType.SYMBOL_PLUS_PLUS))
        local target = parse_register
        statements.add( TokenizerCmdAdd(t,target,TokenizerCmdLiteralInt32(t,1)) )
        return

      elseIf (consume(TokenType.KEYWORD_PUSH))
        if (consume(TokenType.KEYWORD_CH))
          statements.add( CmdStatement(t,"push ch",TokenizerOpcode.PUSH_CH) )
        elseIf (consume(TokenType.KEYWORD_COUNT))
          statements.add( CmdStatement(t,"push count",TokenizerOpcode.PUSH_COUNT) )
        else
          throw peek.error( "Expected 'ch' or 'count'." )
        endIf
        return

      else
        local expr = parse_tokenizer_expression()

        local t2 = peek
        if (consume(TokenType.SYMBOL_PLUS_PLUS))
          statements.add( TokenizerCmdAdd(t,expr,TokenizerCmdLiteralInt32(t2,1)) )
        elseIf (consume(TokenType.SYMBOL_MINUS_MINUS))
          statements.add( TokenizerCmdSubtract(t,expr,TokenizerCmdLiteralInt32(t2,1)) )
        elseIf (consume(TokenType.SYMBOL_PLUS_EQUALS))
          statements.add( TokenizerCmdAdd(t,expr,parse_tokenizer_expression()) )
        elseIf (consume(TokenType.SYMBOL_MINUS_EQUALS))
          statements.add( TokenizerCmdSubtract(t,expr,parse_tokenizer_expression()) )
        else
          statements.add( expr )
        endIf
        return

      endIf

      throw peek.error( "Syntax error - unexpected '$'." (peek->String) )

    method parse_parser_statements( statements:CmdStatements )->Logical
      if (consume_eols)
        parse_multi_line_parser_statements( statements )
        return true
      else
        parse_single_line_parser_statements( statements )
        return false
      endIf

    method parse_multi_line_parser_statements( statements:CmdStatements )
      consume_eols
      while (reader.has_another and not reader.peek.type.is_structural)
        local t = peek
        if (t.type == TokenType.SYMBOL_MINUS or t.type == TokenType.SYMBOL_PLUS) return
        parse_parser_statement( statements, &allow_control_structures )
        while (consume_eols or consume(TokenType.SYMBOL_SEMICOLON)) noAction
      endWhile

    method parse_single_line_parser_statements( statements:CmdStatements )
      while (reader.has_another and not reader.peek.type.is_structural)
        parse_parser_statement( statements, &!allow_control_structures )
        if (not consume(TokenType.SYMBOL_SEMICOLON)) return
        while (consume(TokenType.SYMBOL_SEMICOLON)) noAction

        # Don't let a trailing ';' act as a next-line continuation.
        if (next_is(TokenType.EOL)) escapeWhile
      endWhile

      if (not consume(TokenType.EOL))
        if (not reader.peek.type.is_structural)
          must_consume( TokenType.EOL )  # force an error
        endIf
      endIf

    method read_token_type_def->TokenDef
      local token_type_name : String
      if (next_is(TokenType.STRING))
        local symbol = read.text
        if (Froley.token_defs_by_symbol.contains(symbol))
          return Froley.token_defs_by_symbol[ symbol ]
        else
          throw peek.error( "No token has symbol '$'." (symbol) )
        endIf
      elseIf (next_is(TokenType.INTEGER))
        local symbol = read->Int32->Character->String
        if (Froley.token_defs_by_symbol.contains(symbol))
          return Froley.token_defs_by_symbol[ symbol ]
        else
          throw peek.error( "No token has symbol '$'." (symbol) )
        endIf
      elseIf (next_is(TokenType.IDENTIFIER))
        token_type_name = read_identifier
      else
        throw peek.error( "Token symbol or type name expected." )
      endIf

      if (not Froley.token_defs_by_name.contains(token_type_name))
        throw peek.error( "Invalid token type: '$'." (token_type_name) )
      endIf

      return Froley.token_defs_by_name[ token_type_name ]

    method parse_parser_statement( statements:CmdStatements, &allow_control_structures )
      consume_eols
      if (not has_another) return
      local t = peek
      if (t.type.is_structural) return

      if (consume(TokenType.KEYWORD_ON))
        local token_type_def = read_token_type_def
        if (consume(TokenType.SYMBOL_ARROW))
          local cmd = ParserCmdOnTypeProduce( t, token_type_def, parse_cmd_type(token_type_def) )
          statements.add( cmd )
          return
        else
          consume( TokenType.SYMBOL_COLON )
          local cmd = ParserCmdOnType( t, token_type_def )
          statements.add( cmd )
          if (consume_eols)
            parse_multi_line_parser_statements( cmd.statements )
            t = peek
            while (consume(TokenType.KEYWORD_ELSE_ON))
              cmd.else_on = ParserCmdOnType( t, read_token_type_def )
              cmd = cmd.else_on
              parse_multi_line_parser_statements( cmd.statements )
              t = peek
            endWhile
            if (consume(TokenType.KEYWORD_ELSE))
              cmd.else_on = ParserCmdOnType( t, null )
              cmd = cmd.else_on
              parse_multi_line_parser_statements( cmd.statements )
              t = peek
            endIf
            must_consume( TokenType.KEYWORD_END_ON )
          else
            parse_single_line_parser_statements( cmd.statements )
          endIf
          return
        endIf

      elseIf (consume(TokenType.KEYWORD_SYNTAX_ERROR))
        statements.add( ParserCmdSyntaxError(t) )
        return

      elseIf (consume(TokenType.KEYWORD_BEGIN_LIST))
        statements.add( ParserCmdBeginList(t) )
        return

      elseIf (consume(TokenType.KEYWORD_MUST_CONSUME))
        statements.add( ParserCmdMustConsumeType(t,read_token_type_def) )
        return

      elseIf (consume(TokenType.KEYWORD_PRINTLN))
        if (next_is(TokenType.STRING))
          statements.add( ParserCmdPrintlnString(t,read_string) )
        else
          statements.add( ParserCmdPrintlnCmd(t,parse_parser_expression()) )
        endIf
        return

      elseIf (consume(TokenType.KEYWORD_CREATE))
        statements.add( ParserCmdCreate(t,parse_cmd_type) )
        return

      elseIf (consume(TokenType.KEYWORD_CREATE_NULL))
        statements.add( ParserCmdCreateNull(t) )
        return

      elseIf (consume(TokenType.KEYWORD_CREATE_STATEMENTS))
        statements.add( ParserCmdCreateStatements(t) )
        return

      elseIf (consume(TokenType.KEYWORD_CREATE_ARGS))
        statements.add( ParserCmdCreateArgs(t) )
        return

      elseIf (consume(TokenType.KEYWORD_RETURN))
        statements.add( ParserCmdReturn(t,null) )
        return

      elseIf (consume(TokenType.KEYWORD_TRACE))
        statements.add( ParserCmdTrace(t) )
        return

      elseIf (consume(TokenType.KEYWORD_VAR))
        local name  = read_identifier
        must_consume( TokenType.SYMBOL_EQUALS )
        local value = parse_parser_expression()
        statements.add( ParserCmdWriteVar(t,name,value) )
        return
      endIf

      if (allow_control_structures)
        if (consume(TokenType.KEYWORD_IF))
          local cmd_if = ParserCmdIf( t, parse_parser_expression() )
          if (parse_parser_statements(cmd_if.statements))
            # Multi-line statements
            local outer_if = cmd_if
            while (next_is_parser_end_if(&multi_line))
              consume_eols
              local t2 = read
              local inner_if = ParserCmdIf( t2, parse_parser_expression() )
              parse_multi_line_parser_statements( inner_if.statements )
              ensure outer_if.else_statements
              outer_if.else_statements.add( inner_if )
              outer_if = inner_if
            endWhile
            if (next_is_else(&multi_line))
              consume_eols
              if (next_is(TokenType.KEYWORD_ELSE) and reader.peek(1).type == TokenType.EOL)
                read
                ensure outer_if.else_statements
                parse_multi_line_parser_statements( outer_if.else_statements )
              endIf
            endIf
            must_consume( TokenType.KEYWORD_END_IF )
          else
            # Single line statements
            local outer_if = cmd_if
            while (next_is_parser_end_if(&single_line))
              consume_eols
              local t2 = read
              local inner_if = ParserCmdIf( t2, parse_parser_expression() )
              parse_single_line_parser_statements( inner_if.statements )
              ensure outer_if.else_statements
              outer_if.else_statements.add( inner_if )
              outer_if = inner_if
            endWhile
            if (next_is_else(&single_line))
              consume_eols
              if (next_is(TokenType.KEYWORD_ELSE) and reader.peek(1).type != TokenType.EOL)
                read
                ensure outer_if.else_statements
                parse_single_line_parser_statements( outer_if.else_statements )
              endIf
            endIf
          endIf
          statements.add( cmd_if )
          return

        elseIf (consume(TokenType.KEYWORD_WHILE))
          local cmd_while = ParserCmdWhile( t, parse_parser_expression() )
          if (parse_parser_statements(cmd_while.statements))
            # Returns true if multi-line statements were parsed
            must_consume( TokenType.KEYWORD_END_WHILE )
          endIf
          statements.add( cmd_while )
          return

        endIf
      endIf

      local expr = parse_parser_expression() : Cmd
      t = peek
      if (consume(TokenType.SYMBOL_EQUALS))
        expr = ParserCmdAssign( t, expr, parse_parser_expression() )
      endIf
      statements.add( expr )

      #{
      if (consume(TokenType.KEYWORD_ACCEPT))
        local name = read_identifier
        statements.add( TokenizerCmdAcceptInt32(t,Froley.token_def(name).type) )
        return

      elseIf (consume(TokenType.KEYWORD_CLEAR))
        if (consume(TokenType.KEYWORD_BUFFER))
          statements.add( CmdStatement(t,"clear buffer",TokenizerOpcode.CLEAR_BUFFER) )
          return
        else
          throw peek.error( "Expected 'buffer'." )
        endIf

      elseIf (consume(TokenType.KEYWORD_COLLECT))
        if (next_is(TokenType.KEYWORD_CH))
          must_consume_register_ch
          statements.add( CmdStatement(t,"collect ch",TokenizerOpcode.COLLECT_CH) )
          return
        elseIf (next_is(TokenType.STRING))
          statements.add( TokenizerCmdCollectString(t,read.text) )
          return
        elseIf (next_is(TokenType.INTEGER))
          statements.add( TokenizerCmdCollectCharacter(t,read_integer) )
          return
        else
          throw t.error( "Syntax error - expected 'ch' or a literal string.")
        endIf
        return

      elseIf (consume(TokenType.KEYWORD_DISCARD))
        statements.add( CmdStatement(t,"restart",TokenizerOpcode.RESTART) )
        return

      elseIf (consume(TokenType.KEYWORD_ERROR))
        if (consume(TokenType.KEYWORD_BUFFER))
          statements.add( CmdStatement(t,"error buffer",TokenizerOpcode.ERROR) )
        elseIf (next_is(TokenType.STRING))
          statements.add( CmdStatement(t,"clear buffer",TokenizerOpcode.CLEAR_BUFFER) )
          statements.add( TokenizerCmdCollectString(t,read.text) )
          statements.add( CmdStatement(t,"error <message>",TokenizerOpcode.ERROR) )
        else
          throw peek.error( "Expected 'buffer'." )
        endIf
        return

      elseIf (consume(TokenType.KEYWORD_GOTO))
        statements.add( TokenizerCmdGoto(t,read_identifier) )
        return

      elseIf (consume(TokenType.KEYWORD_HALT))
        statements.add( CmdStatement(t,"halt",TokenizerOpcode.HALT) )
        return

      elseIf (consume(TokenType.KEYWORD_MARK_SRC_POS))
        statements.add( CmdStatement(t,"markSourcePosition",TokenizerOpcode.MARK_SOURCE_POS) )
        return

      elseIf (consume(TokenType.KEYWORD_MODE))
        local label_name = read_label_name
        statements.add( TokenizerCmdMode(t,label_name) )
        return

      elseIf (consume(TokenType.KEYWORD_PRINT))
        if (consume(TokenType.KEYWORD_BUFFER))
          statements.add( CmdStatement(t,"print buffer",TokenizerOpcode.PRINT_BUFFER) )
          return
        elseIf (next_is(TokenType.STRING))
          statements.add( TokenizerCmdPrintString(t,read.text) )
          return
        else
          if (not (next_is(TokenType.EOL) or next_is(TokenType.SYMBOL_SEMICOLON)))
            local expr = parse_parser_expression()
            if (expr.is_ch)
              statements.add( CmdStatement(t,"print ch",TokenizerOpcode.PRINT_CH) )
              return
            elseIf (expr.is_count)
              statements.add( CmdStatement(t,"print count",TokenizerOpcode.PRINT_COUNT) )
              return
            elseIf (expr.is_integer)
              statements.add( TokenizerCmdPrintCharacter(t,expr->Int32) )
              return
            endIf
          endIf
          throw t.error( "Syntax error - valid 'print' arguments are  'ch', 'count', and '<string>'." )
        endIf

      elseIf (consume(TokenType.KEYWORD_RESTART))
        statements.add( CmdStatement(t,"restart",TokenizerOpcode.RESTART) )
        return

      elseIf (consume(TokenType.KEYWORD_RETURN))
        statements.add( CmdStatement(t,"return",TokenizerOpcode.RETURN) )
        return

      elseIf (consume(TokenType.KEYWORD_SCAN_DIGITS))
        throw t.error( "Syntax error. Expected 'ch = scanDigits <min-digits>[..<max-digits>] [base <number-base>]'." )

      elseIf (consume(TokenType.KEYWORD_WHICH))
        local has_parens = consume( TokenType.SYMBOL_OPEN_PAREN )
        local uses_input = consume( TokenType.KEYWORD_INPUT )
        if (not uses_input and not consume(TokenType.KEYWORD_BUFFER))
          throw t.error( "Expected 'which (input)' or 'which (buffer)'." )
        endIf
        if (has_parens) must_consume( TokenType.SYMBOL_CLOSE_PAREN )

        if (uses_input)
          local cmd_which_input = TokenizerCmdWhichInput( t )
          statements.add( cmd_which_input )
          parse_which_input( cmd_which_input )
        else
          # Uses buffer
          local cmd_which = TokenizerCmdWhichBuffer( t )
          statements.add( cmd_which )
          parse_which_buffer( cmd_which )
        endIf
        must_consume( TokenType.KEYWORD_END_WHICH )
        return

      elseIf (next_is(TokenType.SYMBOL_LT))
        local name = read_label_name
        must_consume( TokenType.EOL )
        statements.add( TokenizerCmdLabel(t,name) )
        return

      elseIf (consume(TokenType.SYMBOL_MINUS_MINUS))
        local target = parse_register
        statements.add( TokenizerCmdSubtract(t,target,TokenizerCmdLiteralInt32(t,1)) )
        return

      elseIf (consume(TokenType.SYMBOL_PLUS_PLUS))
        local target = parse_register
        statements.add( TokenizerCmdAdd(t,target,TokenizerCmdLiteralInt32(t,1)) )
        return

      elseIf (consume(TokenType.KEYWORD_PUSH))
        if (consume(TokenType.KEYWORD_CH))
          statements.add( CmdStatement(t,"push ch",TokenizerOpcode.PUSH_CH) )
        elseIf (consume(TokenType.KEYWORD_COUNT))
          statements.add( CmdStatement(t,"push count",TokenizerOpcode.PUSH_COUNT) )
        else
          throw peek.error( "Expected 'ch' or 'count'." )
        endIf
        return

      else
        local expr = parse_parser_expression()

        local t2 = peek
        if (consume(TokenType.SYMBOL_PLUS_PLUS))
          statements.add( TokenizerCmdAdd(t,expr,TokenizerCmdLiteralInt32(t2,1)) )
        elseIf (consume(TokenType.SYMBOL_MINUS_MINUS))
          statements.add( TokenizerCmdSubtract(t,expr,TokenizerCmdLiteralInt32(t2,1)) )
        elseIf (consume(TokenType.SYMBOL_PLUS_EQUALS))
          statements.add( TokenizerCmdAdd(t,expr,parse_parser_expression()) )
        elseIf (consume(TokenType.SYMBOL_MINUS_EQUALS))
          statements.add( TokenizerCmdSubtract(t,expr,parse_parser_expression()) )
        else
          statements.add( expr )
        endIf
        return

      endIf

      throw peek.error( "Syntax error - unexpected '$'." (peek->String) )
    }#

    method parse_cmd_type( token_type_def=null:TokenDef )->CmdType
      local t = peek
      local name = read_identifier
      local args = parse_cmd_init_args( name )
      local symbol = select{ token_type_def:token_type_def.symbol || null as String }
      if (symbol is null)
        symbol = name.after_any( "Cmd" )
        if (symbol.count) symbol = symbol[0].to_lowercase + symbol.from(1)
      endIf
      return Froley.cmd_type( t, name, parse_rule_type, symbol, args )

    method parse_cmd_init_args( cmd_name:String )->CmdTypeProperty[]
      if (not consume(TokenType.SYMBOL_OPEN_PAREN)) return null

      local args = CmdTypeProperty[]
      local first = true
      while (first or consume(TokenType.SYMBOL_COMMA))
        first = false
        args.add( parse_cmd_init_arg )
      endWhile
      must_consume( TokenType.SYMBOL_CLOSE_PAREN )
      store_optional_tag( cmd_name )
      return args

    method parse_cmd_init_arg->CmdTypeProperty
      local name = read_identifier
      local value = ""
      local type = "Cmd"
      if (consume(TokenType.SYMBOL_EQUALS))
        value = read_identifier
        if (consume(TokenType.SYMBOL_DOT)) value += '.' + read_identifier
      endIf
      if (consume(TokenType.SYMBOL_COLON)) type = read_identifier
      return CmdTypeProperty( name, type, value )

    method must_consume_register_ch
      local t = peek
      if (consume(TokenType.KEYWORD_CH)) return
      throw t.error( "Register 'ch' expected." )

    method must_consume_register_count
      local t = peek
      if (consume(TokenType.KEYWORD_COUNT)) return
      throw t.error( "Register 'count' expected." )

    method next_is_tokenizer_end_if( &single_line, &multi_line )->Logical
      local original_pos = reader.position
      consume_eols
      if (not next_is(TokenType.KEYWORD_ELSE_IF))
        reader.position = original_pos
        return false
      endIf

      # Scan to just after the elseIf() conditional
      read  # elseIf
      parse_tokenizer_expression()

      if (single_line)
        local result = not next_is( TokenType.EOL )
        reader.position = original_pos
        return result
      else
        local result = next_is( TokenType.EOL )
        reader.position = original_pos
        return result
      endIf

    method next_is_parser_end_if( &single_line, &multi_line )->Logical
      local original_pos = reader.position
      consume_eols
      if (not next_is(TokenType.KEYWORD_ELSE_IF))
        reader.position = original_pos
        return false
      endIf

      # Scan to just after the elseIf() conditional
      read  # elseIf
      parse_parser_expression()

      if (single_line)
        local result = not next_is( TokenType.EOL )
        reader.position = original_pos
        return result
      else
        local result = next_is( TokenType.EOL )
        reader.position = original_pos
        return result
      endIf

    method next_is_else( &single_line, &multi_line )->Logical
      local original_pos = reader.position
      consume_eols
      if (not next_is(TokenType.KEYWORD_ELSE))
        reader.position = original_pos
        return false
      endIf

      # Scan to just after the else
      read  # else

      if (single_line)
        local result = not next_is( TokenType.EOL )
        reader.position = original_pos
        return result
      else
        local result = next_is( TokenType.EOL )
        reader.position = original_pos
        return result
      endIf

    method parse_register->Cmd
      local t = peek
      if (consume(TokenType.KEYWORD_CH)) return TokenizerCmdRegisterCh( t )
      if (consume(TokenType.KEYWORD_COUNT)) return TokenizerCmdRegisterCount( t )
      throw t.error( "Register 'ch' or 'count' expected." )

    method parse_which_input( cmd_which_input:TokenizerCmdWhichInput )
      #local patterns = (Pattern,ScanState)[]
      consume_eols
      while (has_another and not next_is(TokenType.KEYWORD_END_WHICH))
        if (consume(TokenType.KEYWORD_CASE))
          #{
          if (next_is(TokenType.SYMBOL_OPEN_CURLY))
            local pattern = parse_pattern
            local state = ScanState()
            consume( TokenType.SYMBOL_COLON )
            parse_tokenizer_statements( state.statements )
            patterns.add( (pattern,state) )
          else
            }#
          local expr = parse_tokenizer_expression()
          local node = cmd_which_input.start
          if (expr.is_integer)
            node = node.link( expr->Int32->Character )
          elseIf (expr instanceOf TokenizerCmdLiteralString)
            local st = (expr as TokenizerCmdLiteralString).value
            node = node.link( forEach in st )
          else
            throw expr.t.error( "Syntax error - expected character, character code, or string." )
          endIf
          consume( TokenType.SYMBOL_COLON )
          parse_tokenizer_statements( node.statements )
          #endIf

        elseIf (consume(TokenType.KEYWORD_ACCEPT_ALL))
          local t    = peek
          must_consume( TokenType.SYMBOL_OPEN_BRACKET )
          local name = read_identifier
          must_consume( TokenType.SYMBOL_CLOSE_BRACKET )
          if (Froley.token_defs_by_section.contains(name))
            forEach (def in Froley.section_defs(name))
              if (def.symbol)
                local node = cmd_which_input.start
                node = node.link( forEach in def.symbol )
                node.statements.add( TokenizerCmdAcceptInt32(t,def.type) )
              endIf
            endForEach
          else
            throw t.error( "No token definitions for [$] exist." (name) )
          endIf
        elseIf (consume(TokenType.KEYWORD_DISCARD_ALL))
          local t    = peek
          must_consume( TokenType.SYMBOL_OPEN_BRACKET )
          local name = read_identifier
          must_consume( TokenType.SYMBOL_CLOSE_BRACKET )
          if (Froley.token_defs_by_section.contains(name))
            forEach (def in Froley.section_defs(name))
              if (def.symbol)
                local node = cmd_which_input.start
                node = node.link( forEach in def.symbol )
                node.statements.add( CmdStatement(t,"restart",TokenizerOpcode.RESTART) )
              endIf
            endForEach
          else
            throw t.error( "No token definitions for [$] exist." (name) )
          endIf
        elseIf (consume(TokenType.KEYWORD_OTHERS))
          consume( TokenType.SYMBOL_COLON )
          parse_tokenizer_statements( cmd_which_input.start.statements )
        else
          throw peek.error( "Syntax error - expected one of: 'case', 'acceptIdentifier <TOKEN-TYPE>', 'acceptAll [<section-name>]', 'discardAll [<section-name>]', 'others', 'endWhich'." )
        endIf
        consume_eols
      endWhile

      #forEach ((pattern,state) in patterns)
      #  PatternGraphBuilder( pattern, state ).apply( cmd_which_input.start )
      #endForEach


    method parse_which_buffer( cmd_which:TokenizerCmdWhichBuffer )
      #local patterns = (Pattern,ScanState)[]

      consume_eols
      while (has_another and not next_is(TokenType.KEYWORD_END_WHICH))
        if (consume(TokenType.KEYWORD_CASE))
          #{
          if (next_is(TokenType.SYMBOL_OPEN_CURLY))
            local pattern = parse_pattern
            local state = ScanState()
            consume( TokenType.SYMBOL_COLON )
            parse_tokenizer_statements( state.statements )
            patterns.add( (pattern,state) )
          else
          }#
          local expr = parse_tokenizer_expression()
          local node = cmd_which.start
          if (expr.is_integer)
            node = node.link( expr->Int32->Character )
          elseIf (expr instanceOf TokenizerCmdLiteralString)
            local st = (expr as TokenizerCmdLiteralString).value
            node = node.link( forEach in st )
          else
            throw expr.t.error( "Syntax error - expected character, character code, or string." )
          endIf
          consume( TokenType.SYMBOL_COLON )
          parse_tokenizer_statements( node.statements )
          #endIf

        elseIf (consume(TokenType.KEYWORD_ACCEPT_ALL))
          local t    = peek
          must_consume( TokenType.SYMBOL_OPEN_BRACKET )
          local name = read_identifier
          must_consume( TokenType.SYMBOL_CLOSE_BRACKET )
          if (Froley.token_defs_by_section.contains(name))
            forEach (def in Froley.section_defs(name))
              if (def.symbol)
                local node = cmd_which.start
                node = node.link( forEach in def.symbol )
                node.statements.add( TokenizerCmdAcceptInt32(t,def.type) )
              endIf
            endForEach
          else
            throw t.error( "No token definitions for [$] exist." (name) )
          endIf
        elseIf (consume(TokenType.KEYWORD_DISCARD_ALL))
          local t    = peek
          must_consume( TokenType.SYMBOL_OPEN_BRACKET )
          local name = read_identifier
          must_consume( TokenType.SYMBOL_CLOSE_BRACKET )
          if (Froley.token_defs_by_section.contains(name))
            forEach (def in Froley.section_defs(name))
              if (def.symbol)
                local node = cmd_which.start
                node = node.link( forEach in def.symbol )
                node.statements.add( CmdStatement(t,"restart",TokenizerOpcode.RESTART) )
              endIf
            endForEach
          else
            throw t.error( "No token definitions for [$] exist." (name) )
          endIf
        elseIf (consume(TokenType.KEYWORD_OTHERS))
          consume( TokenType.SYMBOL_COLON )
          parse_tokenizer_statements( cmd_which.start.statements )
        else
          throw peek.error( "Syntax error - expected one of: 'case', 'acceptIdentifier <TOKEN-TYPE>', 'acceptAll [<section-name>]', 'discardAll [<section-name>]', 'others', 'endWhich'." )
        endIf
        consume_eols
      endWhile

      #forEach ((pattern,state) in patterns)
      #  PatternGraphBuilder( pattern, state ).apply( cmd_which.start )
      #endForEach

      #method parse_pattern->Pattern
      #  local t = peek
      #  must_consume( TokenType.SYMBOL_OPEN_CURLY )
      #  return parse_pattern_sequence( t, TokenType.SYMBOL_CLOSE_CURLY )

      #method parse_pattern_sequence( t:Token, close_type:TokenType )->Pattern
      #  local sequence = SequencePattern( t )
      #  while (reader.has_another and not next_is(close_type))
      #    sequence.add( parse_pattern_or )
      #  endWhile
      #  must_consume( close_type )
      #  return sequence

      #method parse_pattern_or->Pattern
      #  local pattern = parse_pattern_multiples
      #  if (not next_is(TokenType.SYMBOL_VERTICAL_BAR)) return pattern

      #  local result = OrPattern( peek )
      #  result.add( pattern )
      #  while (consume(TokenType.SYMBOL_VERTICAL_BAR))
      #    consume_eols
      #    result.add( parse_pattern_multiples )
      #  endWhile
      #  return result

      #method parse_pattern_multiples->Pattern
      #  local pattern = parse_pattern_term
      #  if (next_is(TokenType.SYMBOL_ASTERISK))
      #    return ZeroOrMorePattern( read, pattern )
      #  else
      #    return pattern
      #  endIf

      #method parse_pattern_term->Pattern
      #  local t = peek
      #  if (consume(TokenType.STRING))
      #    return StringPattern( t, t.text )
      #  elseIf (consume(TokenType.KEYWORD_LETTER))
      #    return LetterPattern( t )
      #  elseIf (consume(TokenType.KEYWORD_DIGIT))
      #    return DigitPattern( t )
      #  elseIf (consume(TokenType.INTEGER))
      #    return CharacterPattern( t, t->Int32 )
      #  elseIf (consume(TokenType.SYMBOL_OPEN_PAREN))
      #    return parse_pattern_sequence( t, TokenType.SYMBOL_CLOSE_PAREN )
      #  else
      #    throw t.error( "Syntax error - unexpected '$'." (t->String) )
      #  endIf

      #  #{
      #  if consume( TokenType.SYMBOL_OPEN_PAREN )
      #  local sequence = SequencePattern()
      #  while (reader.has_another and not next_is(TokenType.SYMBOL_CLOSE_CURLY))
      #    sequence.add( parse_pattern_element )
      #  endWhile
      #  must_consume( TokenType.SYMBOL_CLOSE_CURLY )
      #  return sequence
      #  }#

    method read_identifier->String
      if (next_is(TokenType.IDENTIFIER))
        local name = read->String
        store_optional_tag( name )
        return name
      else
        throw peek.error( "Identifier expected." )
      endIf

    method store_optional_tag( name:String )
      #local t = peek
      #if (consume(TokenType.TAG)) Froley.id_tags[ name ] = t

    method read_string->String
      if (next_is(TokenType.STRING))
        return read->String
      else
        throw peek.error( "Literal string expected." )
      endIf

    method read_integer->Int32
      if (next_is(TokenType.INTEGER))
        return read->Int32
      else
        throw peek.error( "Integer value expected." )
      endIf

    method read_label_name->String
      if (next_is(TokenType.IDENTIFIER)) return read_identifier
      must_consume( TokenType.SYMBOL_LT )
      local is_entry_point = consume( TokenType.SYMBOL_LT )
      local name = read->String
      must_consume( TokenType.SYMBOL_GT )
      if (is_entry_point)
        must_consume( TokenType.SYMBOL_GT )
        Froley.entry_points.add( name )
      endIf
      return name

    method parse_parser_methods
      local top_level_method : ParserMethod
      loop
        consume_eols
        if (not has_another) return
        local t = peek
        if (consume(TokenType.SYMBOL_MINUS))
          top_level_method = parse_parser_method( t )
        elseIf (consume(TokenType.SYMBOL_PLUS))
          if (top_level_method is null)
            throw t.error( "Must define a top-level method ('- ...') before defining a nested method." )
          endIf
          local m = parse_parser_method( t )
          top_level_method.nested_method = m
          top_level_method = m
        else
          throw t.error( "Syntax error - unexpected '$'." (t->String) )
        endIf
      endLoop

    method parse_parser_method( t:Token )->ParserMethod
      local m = ParserMethod( t, read_identifier )
      Froley.parser_methods[ m.name ] = m
      parse_rule_type = ParseRuleType.GENERAL

      if (consume(TokenType.SYMBOL_COLON))
        t = peek
        local type_name = read_identifier
        which (type_name)
          case "ParseRule":          m.type = MethodTypeParseRule; parse_rule_type = ParseRuleType.GENERAL
          case "PreUnaryParseRule":  m.type = MethodTypePreUnaryParseRule; parse_rule_type = ParseRuleType.PRE_UNARY
          case "PostUnaryParseRule": m.type = MethodTypePostUnaryParseRule; parse_rule_type = ParseRuleType.POST_UNARY
          case "BinaryParseRule":    m.type = MethodTypeBinaryParseRule; parse_rule_type = ParseRuleType.BINARY
          case "RightAssociativeBinaryParseRule": m.type = MethodTypeRightAssociativeBinaryParseRule; parse_rule_type = ParseRuleType.BINARY
          others
            throw t.error( "No such rule type '$'."(type_name) )
        endWhich
      endIf
      must_consume( TokenType.EOL )

      parse_multi_line_parser_statements( m.statements )

      return m

endClass
