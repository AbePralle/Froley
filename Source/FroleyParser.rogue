module Froley
  uses ParseKit<<Froley>>

class FroleyParser : Parser
  PROPERTIES
    parse_tokenizer_expression : ParseRule
    parse_tokenizer_term       : ParseRule
    parse_parser_expression    : ParseRule
    parse_rule_type            = ParseRuleType.GENERAL : ParseRuleType
    this_method                : ParserMethod

  METHODS
    method init
      # tokenizer expression
      local rule = add( ParseRule("tokenizer_expression") )

      # tokenizer assignment
      rule = add_nested( RightAssociativeBinaryParseRule("assignment") )
      rule.on( "=", <<TokenizerCmdAssign>> )

      # tokenizer logical or
      rule = add_nested( BinaryParseRule("logical_or") )
      rule.on( "or", <<TokenizerCmdLogicalOr>> )

      # tokenizer logical and
      rule = add_nested( BinaryParseRule("logical_and") )
      rule.on( "and", <<TokenizerCmdLogicalAnd>> )

      # tokenizer comparison
      rule = add_nested( BinaryParseRule("comparison") )
      rule.on( "==", <<TokenizerCmdCompareEQ>> )
      rule.on( "!=", <<TokenizerCmdCompareNE>> )
      rule.on( "<",  <<TokenizerCmdCompareLT>> )
      rule.on( "<=", <<TokenizerCmdCompareLE>> )
      rule.on( ">",  <<TokenizerCmdCompareGT>> )
      rule.on( ">=", <<TokenizerCmdCompareGE>> )

      rule = add_nested( PreUnaryParseRule("pre_unary") )
      rule.on( "not", <<TokenizerCmdNot>> )

      # tokenizer is digit, is letter
      rule = add_nested(
        PostUnaryParseRule( "is_digit_is_letter",
          function (rule:ParseRule)->Cmd
            local t = rule.parser.peek
            local term = rule.parser.parse_tokenizer_term()
            local parser = rule.parser
            if (parser.consume(TokenType.KEYWORD_IS))
              if (parser.consume(TokenType.KEYWORD_NOT))
                if (parser.consume(TokenType.KEYWORD_DIGIT))
                  local base = 10
                  if (parser.consume(TokenType.KEYWORD_BASE))
                    if (not parser.next_is(TokenType.INTEGER))
                      throw parser.peek.error( "Base '2'..'16' expected." )
                    endIf
                    base = parser.read->Int32
                  endIf
                  return TokenizerCmdNot( t, term.to_is_digit(base) )
                elseIf (parser.consume(TokenType.KEYWORD_LETTER))
                  return TokenizerCmdNot( t, term.to_is_letter )
                else
                  throw parser.peek.error( "'is not digit' or 'is not letter' expected." )
                endIf
              else
                if (parser.consume(TokenType.KEYWORD_DIGIT))
                  local base = 10
                  if (parser.consume(TokenType.KEYWORD_BASE))
                    if (not parser.next_is(TokenType.INTEGER))
                      throw parser.peek.error( "Base '2'..'16' expected." )
                    endIf
                    base = parser.read->Int32
                  endIf
                  return term.to_is_digit( base )
                elseIf (parser.consume(TokenType.KEYWORD_LETTER))
                  return term.to_is_letter
                else
                  throw parser.peek.error( "'is digit' or 'is letter' expected." )
                endIf
              endIf
            else
              return term
            endIf
          endFunction
        )
      )

      # tokenizer term
      rule = add_nested( ParseRule("tokenizer_term") )
      rule.on( "(",
        function (parser:FroleyParser)->Cmd
          parser.must_consume( TokenType.SYMBOL_OPEN_PAREN )
          local result = parser.parse_tokenizer_expression()
          parser.must_consume( TokenType.SYMBOL_CLOSE_PAREN )
          return result
        endFunction
      )
      rule.on( "@",  
        function(parser:FroleyParser)->Cmd
          local t = parser.read  # '@'
          return TokenizerCmdVarAccess( t, parser.read_identifier )
        endFunction
      )
      rule.on( "buffer",        <<TokenizerCmdRegisterBuffer>> )
      rule.on( "ch",            <<TokenizerCmdRegisterCh>> )
      rule.on( "hasAnother",    <<TokenizerCmdHasAnother>> )
      rule.on( "integer value", <<TokenizerCmdLiteralInt32>> )
      rule.on( "literal string",<<TokenizerCmdLiteralString>> )
      rule.on( "read",          <<TokenizerCmdRead>> )
      rule.on( "identifier",
        function(parser:FroleyParser)->Cmd
          local t = parser.read  # (identifier)
          if (parser.consume(TokenType.SYMBOL_OPEN_PAREN))
            parser.must_consume( TokenType.SYMBOL_CLOSE_PAREN )
            return TokenizerCmdCall( t, t.text )
          else
            return TokenizerCmdAccess( t )
          endIf
        endFunction
      )
      rule.on( "scanIdentifier", <<TokenizerCmdScanIdentfier>> )
      rule.on( "peek",
        function(parser:FroleyParser)->Cmd
          local t = parser.read  # 'peek'
          if (parser.next_is(TokenType.SYMBOL_OPEN_PAREN))
            local expr = parser.parse_tokenizer_expression()
            if (expr.is_integer) return TokenizerCmdPeekInt32( t, expr->Int32 )
            if (expr.is_var)     return TokenizerCmdPeekVar( t, expr->String )
            throw expr.t.error( "Expected 'peek', 'peek(<integer>)', or 'peek(@varname)'." )
          else
            return TokenizerCmdPeek( t )
          endIf
        endFunction
      )
      rule.on( "consume",
        function(parser:FroleyParser)->Cmd
          local t = parser.read  # 'consume'
          if (parser.consume(TokenType.SYMBOL_OPEN_PAREN))
            local expr = parser.parse_tokenizer_expression()
            parser.must_consume( TokenType.SYMBOL_CLOSE_PAREN )
            t = expr.t
            if (expr.is_integer) return TokenizerCmdConsumeCharacter( t, expr->Int32 )
            if (expr.is_string)  return TokenizerCmdConsumeString( t, expr.text )
          endIf
          throw t.error( "Expected 'consume(<character>)' or 'consume(<string>)'." )
        endFunction
      )
      rule.on( "nextIs",
        function(parser:FroleyParser)->Cmd
        local t = parser.read  # 'nextIs'
          if (parser.consume(TokenType.SYMBOL_OPEN_PAREN))
            local expr = parser.parse_tokenizer_expression()
            parser.must_consume( TokenType.SYMBOL_CLOSE_PAREN )
            t = expr.t
            if (expr.is_integer) return TokenizerCmdNextIsCharacter( t, expr->Int32 )
          endIf
          throw t.error( "Expected 'nextIs(<character>)'." )
        endFunction
      )
      rule.on( "modeIs",
        function(parser:FroleyParser)->Cmd
        local t = parser.read  # 'modeIs'
          if (parser.consume(TokenType.SYMBOL_OPEN_PAREN))
            local label_name = parser.read_label_name
            parser.must_consume( TokenType.SYMBOL_CLOSE_PAREN )
            return TokenizerCmdModeIs( t, label_name )
          endIf
          throw t.error( "Expected 'modeIs(<label>)'." )
        endFunction
      )

      # parser expression
      rule = add( ParseRule("parser_expression") )

      # parser logical or
      rule = add_nested( BinaryParseRule("parser_logical_or") )
      rule.on( "or", <<ParserCmdLogicalOr>> )

      # parser logical and
      rule = add_nested( BinaryParseRule("parser_logical_and") )
      rule.on( "and", <<ParserCmdLogicalAnd>> )

      # parser comparison
      rule = add_nested( BinaryParseRule("parser_comparison") )
      rule.on( "==", <<ParserCmdCompareEQ>> )
      rule.on( "!=", <<ParserCmdCompareNE>> )
      rule.on( "<",  <<ParserCmdCompareLT>> )
      rule.on( "<=", <<ParserCmdCompareLE>> )
      rule.on( ">",  <<ParserCmdCompareGT>> )
      rule.on( ">=", <<ParserCmdCompareGE>> )

      rule = add_nested( PreUnaryParseRule("parser_pre_unary") )
      rule.on( "not", <<ParserCmdLogicalNot>> )

      # parser term
      rule = add_nested( ParseRule("parser_term") )
      rule.on( "(",
        function (parser:FroleyParser)->Cmd
          parser.must_consume( TokenType.SYMBOL_OPEN_PAREN )
          local result = parser.parse_parser_expression()
          parser.must_consume( TokenType.SYMBOL_CLOSE_PAREN )
          return result
        endFunction
      )
      rule.on( "integer value",  <<ParserCmdLiteralInt32>> )
      rule.on( "true",  <<ParserCmdLiteralTrue>> )
      rule.on( "false", <<ParserCmdLiteralFalse>> )
      rule.on( "nextHasAttribute",
        function (parser:FroleyParser)->Cmd
          local t = parser.read  # 'nextHasAttribute'
          local name : String
          if (parser.consume(TokenType.SYMBOL_OPEN_PAREN))
            name = parser.read_identifier
            parser.must_consume( TokenType.SYMBOL_CLOSE_PAREN )
          else
            name = parser.read_identifier
          endIf
          if (Froley.token_attributes.contains(name))
            return ParserCmdNextHasAttribute( t, name )
          else
            throw t.error( "No such token attribute '$' (attributes are be defined in an [attributes] section of the .froley file)." (name) )
          endIf
        endFunction
      )
      rule.on( "nextIs",
        function(parser:FroleyParser)->Cmd
          return parser.handle_cmd_with_token_type_def_or_string( (t,d)=>ParserCmdNextIsTokenType(t,d), (t,s)=>ParserCmdNextIsString(t,s) )
        endFunction
      )
      rule.on( "hasAnother",    <<ParserCmdHasAnother>> )
      rule.on( "identifier",
        function(parser:FroleyParser)->Cmd
          local t = parser.read  # (identifier)
          return ParserCmdAccess( t )
          #{
          if (parser.consume(TokenType.SYMBOL_OPEN_PAREN))
            parser.must_consume( TokenType.SYMBOL_CLOSE_PAREN )
            return TokenizerCmdCall( t, t.text )
          else
            return TokenizerCmdAccess( t )
          endIf
          }#
        endFunction
      )
      rule.on( "consume",
        function(parser:FroleyParser)->Cmd
          return parser.handle_cmd_with_token_type_def_or_string( (t,d)=>ParserCmdConsumeTokenType(t,d), (t,s)=>ParserCmdConsumeString(t,s) )
        endFunction
      )

    method parse_tokenizer_statements( statements:CmdStatements )->Logical
      if (consume_eols)
        parse_multi_line_tokenizer_statements( statements )
        return true
      else
        parse_single_line_tokenizer_statements( statements )
        return false
      endIf

    method parse_multi_line_tokenizer_statements( statements:CmdStatements )
      consume_eols
      while (reader.has_another and not reader.peek.type.is_structural)
        parse_tokenizer_statement( statements, &allow_control_structures )
        while (consume_eols or consume(TokenType.SYMBOL_SEMICOLON)) noAction
      endWhile

    method parse_single_line_tokenizer_statements( statements:CmdStatements )
      while (reader.has_another and not reader.peek.type.is_structural)
        parse_tokenizer_statement( statements, &!allow_control_structures )
        if (not consume(TokenType.SYMBOL_SEMICOLON)) return
        while (consume(TokenType.SYMBOL_SEMICOLON)) noAction

        # Don't let a trailing ';' act as a next-line continuation.
        if (next_is(TokenType.EOL)) escapeWhile
      endWhile

      if (not consume(TokenType.EOL))
        if (not reader.peek.type.is_structural)
          must_consume( TokenType.EOL )  # force an error
        endIf
      endIf

    method parse_tokenizer_statement( statements:CmdStatements, &allow_control_structures )
      consume_eols
      if (not has_another) return
      local t = peek
      if (t.type.is_structural) return

      if (allow_control_structures)
        if (consume(TokenType.KEYWORD_IF))
          local cmd_if = TokenizerCmdIf( t, parse_tokenizer_expression() )
          if (parse_tokenizer_statements(cmd_if.statements))
            # Multi-line statements
            local outer_if = cmd_if
            while (next_is_tokenizer_end_if(&multi_line))
              consume_eols
              local t2 = read
              local inner_if = TokenizerCmdIf( t2, parse_tokenizer_expression() )
              parse_multi_line_tokenizer_statements( inner_if.statements )
              ensure outer_if.else_statements
              outer_if.else_statements.add( inner_if )
              outer_if = inner_if
            endWhile
            if (next_is_else(&multi_line))
              consume_eols
              if (next_is(TokenType.KEYWORD_ELSE) and reader.peek(1).type == TokenType.EOL)
                read
                ensure outer_if.else_statements
                parse_multi_line_tokenizer_statements( outer_if.else_statements )
              endIf
            endIf
            must_consume( TokenType.KEYWORD_END_IF )
          else
            # Single line statements
            local outer_if = cmd_if
            while (next_is_tokenizer_end_if(&single_line))
              consume_eols
              local t2 = read
              local inner_if = TokenizerCmdIf( t2, parse_tokenizer_expression() )
              parse_single_line_tokenizer_statements( inner_if.statements )
              ensure outer_if.else_statements
              outer_if.else_statements.add( inner_if )
              outer_if = inner_if
            endWhile
            if (next_is_else(&single_line))
              consume_eols
              if (next_is(TokenType.KEYWORD_ELSE) and reader.peek(1).type != TokenType.EOL)
                read
                ensure outer_if.else_statements
                parse_single_line_tokenizer_statements( outer_if.else_statements )
              endIf
            endIf
          endIf
          statements.add( cmd_if )
          return

        elseIf (consume(TokenType.KEYWORD_WHILE))
          local cmd_while = TokenizerCmdWhile( t, parse_tokenizer_expression() )
          if (parse_tokenizer_statements(cmd_while.statements))
            # Returns true if multi-line statements were parsed
            must_consume( TokenType.KEYWORD_END_WHILE )
          endIf
          statements.add( cmd_while )
          return

        endIf
      endIf

      if (consume(TokenType.KEYWORD_CREATE))
        local name = read_identifier
        statements.add( TokenizerCmdCreateInt32(t,Froley.token_def(name).type) )
        return

      elseIf (consume(TokenType.KEYWORD_MUST_CONSUME))
        if (consume(TokenType.SYMBOL_OPEN_PAREN))
          local expr = parse_tokenizer_expression()
          must_consume( TokenType.SYMBOL_CLOSE_PAREN )
          t = expr.t
          if (expr.is_integer)
            statements.add TokenizerCmdMustConsumeCharacter( t, expr->Int32 )
            return
          endIf
        else
          local expr = parse_tokenizer_expression()
          t = expr.t
          if (expr.is_integer)
            statements.add TokenizerCmdMustConsumeCharacter( t, expr->Int32 )
            return
          endIf
        endIf
        throw t.error( "Expected 'mustConsume(<character>)'." )

      elseIf (consume(TokenType.KEYWORD_PRODUCE))
        local name = read_identifier
        statements.add( TokenizerCmdCreateInt32(t,Froley.token_def(name).type) )
        statements.add( TokenizerCmdOp(t,"restart",TokenizerOpcode.RESTART) )
        return

      elseIf (consume(TokenType.KEYWORD_CLEAR))
        if (consume(TokenType.KEYWORD_BUFFER))
          statements.add( TokenizerCmdOp(t,"clear buffer",TokenizerOpcode.CLEAR_BUFFER) )
          return
        else
          throw peek.error( "Expected 'buffer'." )
        endIf

      elseIf (consume(TokenType.KEYWORD_BUFFER_TO_LC))
        statements.add( TokenizerCmdOp(t,"bufferToLowercase",TokenizerOpcode.BUFFER_TO_LOWERCASE) )
        return

      elseIf (consume(TokenType.KEYWORD_COLLECT))
        if (next_is(TokenType.KEYWORD_CH))
          must_consume_register_ch
          statements.add( TokenizerCmdOp(t,"collect ch",TokenizerOpcode.COLLECT_CH) )
          return
        elseIf (next_is(TokenType.STRING))
          statements.add( TokenizerCmdCollectString(t,read.text) )
          return
        elseIf (next_is(TokenType.INTEGER))
          statements.add( TokenizerCmdCollectCharacter(t,read_integer) )
          return
        else
          throw t.error( "Syntax error - expected 'ch' or a literal string.")
        endIf
        return

      elseIf (consume(TokenType.KEYWORD_DISCARD))
        statements.add( TokenizerCmdOp(t,"restart",TokenizerOpcode.RESTART) )
        return

      elseIf (consume(TokenType.KEYWORD_SYNTAX_ERROR))
        if (consume(TokenType.KEYWORD_BUFFER))
          statements.add( TokenizerCmdOp(t,"syntaxError buffer",TokenizerOpcode.SYNTAX_ERROR_BUFFER) )
        elseIf (next_is(TokenType.STRING))
          statements.add( TokenizerCmdOp(t,"clear buffer",TokenizerOpcode.CLEAR_BUFFER) )
          statements.add( TokenizerCmdCollectString(t,read.text) )
          statements.add( TokenizerCmdOp(t,"syntaxError <message>",TokenizerOpcode.SYNTAX_ERROR_BUFFER) )
        else
          statements.add( TokenizerCmdOp(t,"syntaxError",TokenizerOpcode.SYNTAX_ERROR) )
        endIf
        return

      elseIf (consume(TokenType.KEYWORD_GOTO))
        statements.add( TokenizerCmdGoto(t,read_identifier) )
        return

      elseIf (consume(TokenType.KEYWORD_HALT))
        statements.add( TokenizerCmdOp(t,"halt",TokenizerOpcode.HALT) )
        return

      elseIf (consume(TokenType.KEYWORD_MARK_SRC_POS))
        statements.add( TokenizerCmdOp(t,"markSourcePosition",TokenizerOpcode.MARK_SOURCE_POS) )
        return

      elseIf (consume(TokenType.KEYWORD_MODE))
        local label_name = read_label_name
        statements.add( TokenizerCmdMode(t,label_name) )
        return

      elseIf (consume(TokenType.KEYWORD_NO_ACTION))
        return

      elseIf (consume(TokenType.KEYWORD_PRINT))
        if (consume(TokenType.KEYWORD_BUFFER))
          statements.add( TokenizerCmdOp(t,"print buffer",TokenizerOpcode.PRINT_BUFFER) )
          return
        elseIf (next_is(TokenType.STRING))
          statements.add( TokenizerCmdPrintString(t,read.text) )
          return
        else
          if (not (next_is(TokenType.EOL) or next_is(TokenType.SYMBOL_SEMICOLON)))
            local expr = parse_tokenizer_expression()
            if (expr.is_ch)
              statements.add( TokenizerCmdOp(t,"print ch",TokenizerOpcode.PRINT_CH) )
              return
            elseIf (expr.is_var)
              statements.add( TokenizerCmdVarOp(t,"print @$"(expr),TokenizerOpcode.PRINT_VAR,expr->String) )
              return
            elseIf (expr.is_integer)
              statements.add( TokenizerCmdPrintCharacter(t,expr->Int32) )
              return
            endIf
          endIf
          throw t.error( "Syntax error - valid 'print' arguments are  'ch', 'count', 'var', and '<string>'." )
        endIf

      elseIf (consume(TokenType.KEYWORD_RESTART))
        if (next_is(TokenType.IDENTIFIER))
          local t2 = peek
          statements.add( TokenizerCmdMode(t2,read_identifier) )
        endIf
        statements.add( TokenizerCmdOp(t,"restart",TokenizerOpcode.RESTART) )
        return

      elseIf (consume(TokenType.KEYWORD_RETURN))
        statements.add( TokenizerCmdOp(t,"return",TokenizerOpcode.RETURN) )
        return

      elseIf (consume(TokenType.KEYWORD_TRACE))
        statements.add( TokenizerCmdTrace(t) )
        return

      elseIf (consume(TokenType.KEYWORD_TRIM))
        if (consume(TokenType.KEYWORD_BUFFER))
          statements.add( TokenizerCmdOp(t,"trim buffer",TokenizerOpcode.TRIM_BUFFER) )
          return
        else
          throw peek.error( "Expected 'buffer'." )
        endIf

      elseIf (consume(TokenType.KEYWORD_WHICH))
        local has_parens = consume( TokenType.SYMBOL_OPEN_PAREN )
        local uses_input = consume( TokenType.KEYWORD_INPUT )
        if (not uses_input and not consume(TokenType.KEYWORD_BUFFER))
          throw t.error( "Expected 'which (input)' or 'which (buffer)'." )
        endIf
        if (has_parens) must_consume( TokenType.SYMBOL_CLOSE_PAREN )

        if (uses_input)
          local cmd_which_input = TokenizerCmdWhichInput( t )
          statements.add( cmd_which_input )
          parse_which_input( cmd_which_input )
        else
          # Uses buffer
          local cmd_which = TokenizerCmdWhichBuffer( t )
          statements.add( cmd_which )
          parse_which_buffer( cmd_which )
        endIf
        must_consume( TokenType.KEYWORD_END_WHICH )
        return

      elseIf (next_is(TokenType.SYMBOL_LT))
        local name = read_label_name
        must_consume( TokenType.EOL )
        statements.add( TokenizerCmdLabel(t,name) )
        return

      elseIf (consume(TokenType.SYMBOL_MINUS_MINUS))
        local target = parse_register
        statements.add( TokenizerCmdSubtract(t,target,TokenizerCmdLiteralInt32(t,1)) )
        return

      elseIf (consume(TokenType.SYMBOL_PLUS_PLUS))
        local target = parse_register
        statements.add( TokenizerCmdAdd(t,target,TokenizerCmdLiteralInt32(t,1)) )
        return

      else
        local expr = parse_tokenizer_expression()

        local t2 = peek
        if (consume(TokenType.SYMBOL_PLUS_PLUS))
          statements.add( TokenizerCmdAdd(t,expr,TokenizerCmdLiteralInt32(t2,1)) )
        elseIf (consume(TokenType.SYMBOL_MINUS_MINUS))
          statements.add( TokenizerCmdSubtract(t,expr,TokenizerCmdLiteralInt32(t2,1)) )
        elseIf (consume(TokenType.SYMBOL_PLUS_EQUALS))
          statements.add( TokenizerCmdAdd(t,expr,parse_tokenizer_expression()) )
        elseIf (consume(TokenType.SYMBOL_MINUS_EQUALS))
          statements.add( TokenizerCmdSubtract(t,expr,parse_tokenizer_expression()) )
        else
          statements.add( expr )
        endIf
        return

      endIf

      throw peek.error( "Syntax error - unexpected '$'." (peek->String) )

    method parse_parser_statements( statements:CmdStatements )->Logical
      if (consume_eols)
        parse_multi_line_parser_statements( statements )
        return true
      else
        parse_single_line_parser_statements( statements )
        return false
      endIf

    method parse_multi_line_parser_statements( statements:CmdStatements )
      consume_eols
      while (reader.has_another and not reader.peek.type.is_structural)
        local t = peek
        if (t.type == TokenType.SYMBOL_MINUS or t.type == TokenType.SYMBOL_PLUS) return
        parse_parser_statement( statements, &allow_control_structures )
        while (consume_eols or consume(TokenType.SYMBOL_SEMICOLON)) noAction
      endWhile

    method parse_single_line_parser_statements( statements:CmdStatements )
      while (reader.has_another and not reader.peek.type.is_structural)
        parse_parser_statement( statements, &!allow_control_structures )
        if (not consume(TokenType.SYMBOL_SEMICOLON)) return
        while (consume(TokenType.SYMBOL_SEMICOLON)) noAction

        # Don't let a trailing ';' act as a next-line continuation.
        if (next_is(TokenType.EOL)) escapeWhile
      endWhile

      if (not consume(TokenType.EOL))
        if (not reader.peek.type.is_structural)
          must_consume( TokenType.EOL )  # force an error
        endIf
      endIf

    method read_token_type_def_for_string( symbol:String )->TokenDef
      if (Froley.token_defs_by_symbol.contains(symbol))
        return Froley.token_defs_by_symbol[ symbol ]
      else
        return null
      endIf

    method read_token_type_def->TokenDef
      local token_type_name : String
      if (next_is(TokenType.STRING))
        local symbol = read.text
        local result = read_token_type_def_for_string( symbol )
        if (result) return result
        throw peek.error( "No token has symbol '$'." (symbol) )
      elseIf (next_is(TokenType.INTEGER))
        local symbol = read->Int32->Character->String
        if (Froley.token_defs_by_symbol.contains(symbol))
          return Froley.token_defs_by_symbol[ symbol ]
        else
          throw peek.error( "No token has symbol '$'." (symbol) )
        endIf
      elseIf (next_is(TokenType.IDENTIFIER))
        token_type_name = read_identifier
      else
        throw peek.error( "Token symbol or type name expected." )
      endIf

      if (not Froley.token_defs_by_name.contains(token_type_name))
        throw peek.error( "Invalid token type: '$'." (token_type_name) )
      endIf

      return Froley.token_defs_by_name[ token_type_name ]

    method handle_cmd_with_token_type_def_or_string( fn_if_token_type:Function(Token,TokenDef)->Cmd, fn_if_string:Function(Token,String)->Cmd )->Cmd
      local t = read  # keyword
      local has_parens = consume( TokenType.SYMBOL_OPEN_PAREN )
      local result : Cmd
      if (consume(TokenType.SYMBOL_AT))
        result = fn_if_string( t, read_string )
      else
        result = fn_if_token_type( t, read_token_type_def )
      endIf
      if (has_parens) must_consume( TokenType.SYMBOL_CLOSE_PAREN )
      return result

    method parse_parser_statement( statements:CmdStatements, &allow_control_structures )
      consume_eols
      if (not has_another) return
      local t = peek
      if (t.type.is_structural) return

      local is_peek = next_is( TokenType.KEYWORD_ON_PEEK )
      if (consume(TokenType.KEYWORD_ON) or consume(TokenType.KEYWORD_ON_PEEK))
        local token_type_def = read_token_type_def
        if (consume(TokenType.SYMBOL_ARROW))
          consume_eols
          local cmd = ParserCmdOnTypeProduce( t, token_type_def, parse_cmd_type(token_type_def) )
          if (is_peek) cmd.peek = true
          statements.add( cmd )
          return
        else
          local on_commands = parse_on_commands
          consume( TokenType.SYMBOL_COLON )
          local cmd = ParserCmdOnType( t, token_type_def )
          if (is_peek) cmd.peek = true
          statements.add( cmd )

          if (on_commands)
            cmd.statements.add( on_commands )

            t = peek
            if (consume(TokenType.SYMBOL_ARROW))
              consume_eols
              cmd.statements.add( ParserCmdCreate(t,parse_cmd_type) )
              cmd.statements.add( ParserCmdReturn(t,null) )
              return
            endIf
          endIf

          if ((not on_commands or on_commands.is_empty) and consume_eols)
            parse_multi_line_parser_statements( cmd.statements )
            t = peek
            is_peek = next_is( TokenType.KEYWORD_ELSE_ON_PEEK )
            while (consume(TokenType.KEYWORD_ELSE_ON) or consume(TokenType.KEYWORD_ELSE_ON_PEEK))
              cmd.else_on = ParserCmdOnType( t, read_token_type_def )
              if (is_peek) cmd.else_on.peek = true
              on_commands = parse_on_commands
              if (on_commands) cmd.statements.add( on_commands )
              cmd = cmd.else_on
              parse_multi_line_parser_statements( cmd.statements )
              t = peek
              is_peek = next_is( TokenType.KEYWORD_ELSE_ON_PEEK )
            endWhile
            if (consume(TokenType.KEYWORD_ELSE))
              cmd.else_on = ParserCmdOnType( t, null )
              cmd = cmd.else_on
              parse_multi_line_parser_statements( cmd.statements )
              t = peek
            endIf
            must_consume( TokenType.KEYWORD_END_ON )
          else
            parse_single_line_parser_statements( cmd.statements )
          endIf
          return
        endIf

      elseIf (consume(TokenType.KEYWORD_SYNTAX_ERROR))
        if (next_is(TokenType.STRING))
          statements.add( ParserCmdSyntaxError(t,read_string) )
        else
          statements.add( ParserCmdSyntaxError(t) )
        endIf
        return

      elseIf (consume(TokenType.KEYWORD_BEGIN_LIST))
        statements.add( ParserCmdBeginList(t) )
        return

      elseIf (next_is(TokenType.KEYWORD_MUST_CONSUME))
        statements.add( handle_cmd_with_token_type_def_or_string( (t,d)=>ParserCmdMustConsumeType(t,d), (t,s)=>ParserCmdMustConsumeString(t,s) ) )
        return

      elseIf (consume(TokenType.KEYWORD_NO_ACTION))
        return

      elseIf (consume(TokenType.KEYWORD_PRINTLN))
        if (next_is(TokenType.STRING))
          statements.add( ParserCmdPrintlnString(t,read_string) )
        else
          statements.add( ParserCmdPrintlnCmd(t,parse_parser_expression()) )
        endIf
        return

      elseIf (consume(TokenType.KEYWORD_CREATE))
        statements.add( ParserCmdCreate(t,parse_cmd_type) )
        return

      elseIf (consume(TokenType.KEYWORD_CREATE_NULL))
        statements.add( ParserCmdCreateNull(t) )
        return

      elseIf (consume(TokenType.KEYWORD_CREATE_LIST))
        local list_type : CmdType
        if (next_is(TokenType.IDENTIFIER)) list_type = parse_cmd_type( &base_type="CmdList", &parse_rule_type=ParseRuleType.LIST )
        else                               list_type = Froley.cmd_type( t, "CmdList", ParseRuleType.LIST )
        statements.add( ParserCmdCreateList(t,list_type) )
        return

      elseIf (consume(TokenType.KEYWORD_DISCARD_LIST))
        statements.add( ParserCmdDiscardList(t) )
        statements.add( ParserCmdReturn(t,null) )
        return

      elseIf (consume(TokenType.KEYWORD_PRODUCE))
        statements.add( ParserCmdCreate(t,parse_cmd_type) )
        statements.add( ParserCmdReturn(t,null) )
        return

      elseIf (consume(TokenType.KEYWORD_PRODUCE_NULL))
        statements.add( ParserCmdCreateNull(t) )
        statements.add( ParserCmdReturn(t,null) )
        return

      elseIf (consume(TokenType.KEYWORD_PRODUCE_LIST))
        local list_type : CmdType
        if (next_is(TokenType.IDENTIFIER)) list_type = parse_cmd_type( &base_type="CmdList", &parse_rule_type=ParseRuleType.LIST )
        else                               list_type = Froley.cmd_type( t, "CmdList", ParseRuleType.LIST )
        statements.add( ParserCmdCreateList(t,list_type) )
        statements.add( ParserCmdReturn(t,null) )
        return

      elseIf (consume(TokenType.KEYWORD_RETURN))
        statements.add( ParserCmdReturn(t,null) )
        return

      elseIf (consume(TokenType.KEYWORD_SAVE_POSITION))
        statements.add( ParserCmdSavePosition(t) )
        return

      elseIf (consume(TokenType.KEYWORD_RESTORE_POSITION))
        statements.add( ParserCmdRestorePosition(t) )
        return

      elseIf (consume(TokenType.KEYWORD_DISCARD_POSITION))
        statements.add( ParserCmdDiscardPosition(t) )
        return

      elseIf (consume(TokenType.KEYWORD_SWAP))
        statements.add( ParserCmdOp(t,"swap",ParserOpcode.SWAP) )
        return

      elseIf (consume(TokenType.KEYWORD_TRACE))
        statements.add( ParserCmdTrace(t) )
        return

      elseIf (consume(TokenType.KEYWORD_VAR))
        local name  = read_identifier
        must_consume( TokenType.SYMBOL_EQUALS )
        local value = parse_parser_expression()
        statements.add( ParserCmdDeclareVar(t,name,value) )
        return
      endIf

      if (allow_control_structures)
        if (consume(TokenType.KEYWORD_IF))
          local cmd_if = ParserCmdIf( t, parse_parser_expression() )
          if (parse_parser_statements(cmd_if.statements))
            # Multi-line statements
            local outer_if = cmd_if
            while (next_is_parser_end_if(&multi_line))
              consume_eols
              local t2 = read
              local inner_if = ParserCmdIf( t2, parse_parser_expression() )
              parse_multi_line_parser_statements( inner_if.statements )
              ensure outer_if.else_statements
              outer_if.else_statements.add( inner_if )
              outer_if = inner_if
            endWhile
            if (next_is_else(&multi_line))
              consume_eols
              if (next_is(TokenType.KEYWORD_ELSE) and reader.peek(1).type == TokenType.EOL)
                read
                ensure outer_if.else_statements
                parse_multi_line_parser_statements( outer_if.else_statements )
              endIf
            endIf
            must_consume( TokenType.KEYWORD_END_IF )
          else
            # Single line statements
            local outer_if = cmd_if
            while (next_is_parser_end_if(&single_line))
              consume_eols
              local t2 = read
              local inner_if = ParserCmdIf( t2, parse_parser_expression() )
              parse_single_line_parser_statements( inner_if.statements )
              ensure outer_if.else_statements
              outer_if.else_statements.add( inner_if )
              outer_if = inner_if
            endWhile
            if (next_is_else(&single_line))
              consume_eols
              if (next_is(TokenType.KEYWORD_ELSE) and reader.peek(1).type != TokenType.EOL)
                read
                ensure outer_if.else_statements
                parse_single_line_parser_statements( outer_if.else_statements )
              endIf
            endIf
          endIf
          statements.add( cmd_if )
          return

        elseIf (consume(TokenType.KEYWORD_WHILE))
          local cmd_while = ParserCmdWhile( t, parse_parser_expression() )
          if (parse_parser_statements(cmd_while.statements))
            # Returns true if multi-line statements were parsed
            must_consume( TokenType.KEYWORD_END_WHILE )
          endIf
          statements.add( cmd_while )
          return

        endIf
      endIf

      local expr = parse_parser_expression() : Cmd
      t = peek
      if (consume(TokenType.SYMBOL_EQUALS))
        expr = ParserCmdAssign( t, expr, parse_parser_expression() )
      endIf
      statements.add( expr )

    method parse_on_commands->CmdStatements
      local commands : CmdStatements
      local work_list = CmdStatements()

      while (consume(TokenType.SYMBOL_COMMA))
        ensure commands
        if (next_is(TokenType.STRING))
          local t = peek
          local symbol = read_string
          local def = Froley.token_defs_by_symbol[ symbol ]
          if (def) commands.add( ParserCmdMustConsumeType(t,def) )
          else     throw t.error( "No token has symbol '$'." (symbol) )
        else
          parse_parser_statement( work_list.[clear] )
          forEach (cmd in work_list)
            if (cmd instanceOf ParserCmdLiteralInt32)
              local symbol = (cmd as ParserCmdLiteralInt32).value->Character->String
              local def = Froley.token_defs_by_symbol[ symbol ]
              if (def) commands.add( ParserCmdMustConsumeType(cmd.t,def) )
              else     throw cmd.t.error( "No token has symbol '$'." (symbol) )
            elseIf (cmd instanceOf ParserCmdAccess)
              local name = (cmd as ParserCmdAccess).name
              local def = Froley.token_defs_by_name[ name ]
              if (def) commands.add( ParserCmdMustConsumeType(cmd.t,def) )
              else     commands.add( cmd )
            else
              commands.add( cmd )
            endIf
          endForEach
        endIf
      endWhile

      return commands

    method parse_cmd_type( token_type_def=null:TokenDef, base_type=null:String, parse_rule_type=null:ParseRuleType? )->CmdType
      local t = peek
      local name = read_identifier
      local args = parse_cmd_init_args( name )
      local symbol = which{ token_type_def:token_type_def.symbol || null as String }
      if (symbol is null)
        symbol = name.after_any( "Cmd" )
      endIf

      if (consume(TokenType.SYMBOL_COLON))
        base_type = read_identifier
      endIf

      if (not parse_rule_type) parse_rule_type = this.parse_rule_type

      return Froley.cmd_type( t, name, parse_rule_type.value, symbol, args, base_type )

    method parse_cmd_init_args( cmd_name:String )->CmdTypeProperty[]
      if (not consume(TokenType.SYMBOL_OPEN_PAREN)) return null

      consume_eols

      local args = CmdTypeProperty[]
      local first = true
      while (first or consume(TokenType.SYMBOL_COMMA))
        first = false
        consume_eols
        args.add( parse_cmd_init_arg )
        consume_eols
      endWhile
      must_consume( TokenType.SYMBOL_CLOSE_PAREN )
      store_optional_tag( cmd_name )
      return args

    method parse_cmd_init_arg->CmdTypeProperty
      local name = read_identifier
      local value = ""
      local type = "Cmd"
      if (consume(TokenType.SYMBOL_EQUALS))
        local t = read
        if (t.text) value = t.text
        else        value = t.number->String
        if (consume(TokenType.SYMBOL_DOT)) value += '.' + read_identifier
      endIf
      if (value == "t") value = "t.content"
      if (consume(TokenType.SYMBOL_COLON)) type = read_identifier
      return CmdTypeProperty( name, type, value )

    method must_consume_register_ch
      local t = peek
      if (consume(TokenType.KEYWORD_CH)) return
      throw t.error( "Register 'ch' expected." )

    method next_is_tokenizer_end_if( &single_line, &multi_line )->Logical
      local original_pos = reader.position
      consume_eols
      if (not next_is(TokenType.KEYWORD_ELSE_IF))
        reader.position = original_pos
        return false
      endIf

      # Scan to just after the elseIf() conditional
      read  # elseIf
      parse_tokenizer_expression()

      if (single_line)
        local result = not next_is( TokenType.EOL )
        reader.position = original_pos
        return result
      else
        local result = next_is( TokenType.EOL )
        reader.position = original_pos
        return result
      endIf

    method next_is_parser_end_if( &single_line, &multi_line )->Logical
      local original_pos = reader.position
      consume_eols
      if (not next_is(TokenType.KEYWORD_ELSE_IF))
        reader.position = original_pos
        return false
      endIf

      # Scan to just after the elseIf() conditional
      read  # elseIf
      parse_parser_expression()

      if (single_line)
        local result = not next_is( TokenType.EOL )
        reader.position = original_pos
        return result
      else
        local result = next_is( TokenType.EOL )
        reader.position = original_pos
        return result
      endIf

    method next_is_else( &single_line, &multi_line )->Logical
      local original_pos = reader.position
      consume_eols
      if (not next_is(TokenType.KEYWORD_ELSE))
        reader.position = original_pos
        return false
      endIf

      # Scan to just after the else
      read  # else

      if (single_line)
        local result = not next_is( TokenType.EOL )
        reader.position = original_pos
        return result
      else
        local result = next_is( TokenType.EOL )
        reader.position = original_pos
        return result
      endIf

    method parse_register->Cmd
      local t = peek
      if (consume(TokenType.KEYWORD_CH)) return TokenizerCmdRegisterCh( t )
      if (consume(TokenType.SYMBOL_AT)) return TokenizerCmdVarAccess( t, read_identifier )
      throw t.error( "Register 'ch' or variable '@varname' expected." )

    method parse_which_input( cmd_which_input:TokenizerCmdWhichInput )
      consume_eols
      while (has_another and not next_is(TokenType.KEYWORD_END_WHICH))
        if (consume(TokenType.KEYWORD_CASE))
          local expr = parse_tokenizer_expression()
          local cmd = cmd_which_input.start
          if (expr.is_integer)
            cmd = cmd.link( expr->Int32->Character )
          elseIf (expr instanceOf TokenizerCmdLiteralString)
            local st = (expr as TokenizerCmdLiteralString).value
            cmd = cmd.link( forEach in st )
          else
            throw expr.t.error( "Syntax error - expected character, character code, or string." )
          endIf
          consume( TokenType.SYMBOL_COLON )
          parse_tokenizer_statements( cmd.statements )

        elseIf (consume(TokenType.KEYWORD_PRODUCE_ANY))
          local t    = peek
          must_consume( TokenType.SYMBOL_OPEN_BRACKET )
          local name = read_identifier
          must_consume( TokenType.SYMBOL_CLOSE_BRACKET )
          if (Froley.token_defs_by_section.contains(name))
            forEach (def in Froley.section_defs(name))
              if (def.symbol)
                local cmd = cmd_which_input.start
                cmd = cmd.link( forEach in def.symbol )
                cmd.statements.add( TokenizerCmdCreateInt32(t,def.type) )
                cmd.statements.add( TokenizerCmdOp(t,"restart",TokenizerOpcode.RESTART) )
              endIf
            endForEach
          else
            throw t.error( "No token definitions for [$] exist." (name) )
          endIf
        elseIf (consume(TokenType.KEYWORD_DISCARD_ANY))
          local t    = peek
          must_consume( TokenType.SYMBOL_OPEN_BRACKET )
          local name = read_identifier
          must_consume( TokenType.SYMBOL_CLOSE_BRACKET )
          if (Froley.token_defs_by_section.contains(name))
            forEach (def in Froley.section_defs(name))
              if (def.symbol)
                local cmd = cmd_which_input.start
                cmd = cmd.link( forEach in def.symbol )
                cmd.statements.add( TokenizerCmdOp(t,"restart",TokenizerOpcode.RESTART) )
              endIf
            endForEach
          else
            throw t.error( "No token definitions for [$] exist." (name) )
          endIf
        elseIf (consume(TokenType.KEYWORD_OTHERS))
          consume( TokenType.SYMBOL_COLON )
          parse_tokenizer_statements( cmd_which_input.start.statements )
        else
          throw peek.error( "Syntax error - expected one of: 'produceAny [<section-name>]', 'discardAny [<section-name>]', 'case', 'others', 'endWhich'." )
        endIf
        consume_eols
      endWhile

    method parse_which_buffer( cmd_which:TokenizerCmdWhichBuffer )
      consume_eols
      while (has_another and not next_is(TokenType.KEYWORD_END_WHICH))
        if (consume(TokenType.KEYWORD_CASE))
          local ignore_case = consume( TokenType.SYMBOL_TILDE )
          local expr = parse_tokenizer_expression()
          local cmd = cmd_which.start
          if (expr.is_integer)
            local ch = expr->Int32->Character
            local uc = ch.to_uppercase
            local lc = ch.to_lowercase
            if (ignore_case and uc != lc)
              cmd = cmd.link( uc, cmd.link(lc) )
            else
              cmd = cmd.link( ch )
            endIf
          elseIf (expr instanceOf TokenizerCmdLiteralString)
            local st = (expr as TokenizerCmdLiteralString).value
            if (st.count) cmd = cmd.link( st, &=ignore_case )
          else
            throw expr.t.error( "Syntax error - expected character, character code, or string." )
          endIf
          consume( TokenType.SYMBOL_COLON )
          parse_tokenizer_statements( cmd.statements )

        elseIf (consume(TokenType.KEYWORD_PRODUCE_ANY))
          local t    = peek
          must_consume( TokenType.SYMBOL_OPEN_BRACKET )
          local name = read_identifier
          must_consume( TokenType.SYMBOL_CLOSE_BRACKET )
          if (Froley.token_defs_by_section.contains(name))
            forEach (def in Froley.section_defs(name))
              if (def.symbol)
                local cmd = cmd_which.start
                cmd = cmd.link( forEach in def.symbol )
                cmd.statements.add( TokenizerCmdCreateInt32(t,def.type) )
                cmd.statements.add( TokenizerCmdOp(t,"restart",TokenizerOpcode.RESTART) )
              endIf
            endForEach
          else
            throw t.error( "No token definitions for [$] exist." (name) )
          endIf
        elseIf (consume(TokenType.KEYWORD_DISCARD_ANY))
          local t    = peek
          must_consume( TokenType.SYMBOL_OPEN_BRACKET )
          local name = read_identifier
          must_consume( TokenType.SYMBOL_CLOSE_BRACKET )
          if (Froley.token_defs_by_section.contains(name))
            forEach (def in Froley.section_defs(name))
              if (def.symbol)
                local cmd = cmd_which.start
                cmd = cmd.link( forEach in def.symbol )
                cmd.statements.add( TokenizerCmdOp(t,"restart",TokenizerOpcode.RESTART) )
              endIf
            endForEach
          else
            throw t.error( "No token definitions for [$] exist." (name) )
          endIf
        elseIf (consume(TokenType.KEYWORD_OTHERS))
          consume( TokenType.SYMBOL_COLON )
          parse_tokenizer_statements( cmd_which.start.statements )
        else
          throw peek.error( "Syntax error - expected one of: 'produceAny [<section-name>]', 'discardAny [<section-name>]', 'case', 'others', 'endWhich'." )
        endIf
        consume_eols
      endWhile

    method read_identifier->String
      if (next_is(TokenType.IDENTIFIER))
        local name = read->String
        store_optional_tag( name )
        return name
      else
        throw peek.error( "Identifier expected." )
      endIf

    method store_optional_tag( name:String )
      #local t = peek
      #if (consume(TokenType.TAG)) Froley.id_tags[ name ] = t

    method read_string( &optional )->String
      if (next_is(TokenType.STRING))
        return read.text
      else
        if (optional) return null
        throw peek.error( "Literal string expected." )
      endIf

    method read_integer->Int32
      if (next_is(TokenType.INTEGER))
        return read->Int32
      else
        throw peek.error( "Integer value expected." )
      endIf

    method read_label_name->String
      if (next_is(TokenType.IDENTIFIER)) return read_identifier
      must_consume( TokenType.SYMBOL_LT )
      local is_entry_point = consume( TokenType.SYMBOL_LT )
      local name = read->String
      must_consume( TokenType.SYMBOL_GT )
      if (is_entry_point)
        must_consume( TokenType.SYMBOL_GT )
        Froley.entry_points.add( name )
      endIf
      return name

    method parse_parser_methods
      local top_level_method : ParserMethod
      loop
        consume_eols
        if (not has_another) return
        local t = peek
        if (consume(TokenType.SYMBOL_MINUS))
          top_level_method = parse_parser_method( t )
        elseIf (consume(TokenType.SYMBOL_PLUS))
          if (top_level_method is null)
            throw t.error( "Must define a top-level method ('- ...') before defining a nested method." )
          endIf
          local m = parse_parser_method( t )
          top_level_method.nested_method = m
          top_level_method = m
        else
          throw t.error( "Syntax error - unexpected '$'." (t->String) )
        endIf
      endLoop

    method parse_parser_method( t:Token )->ParserMethod
      local m = ParserMethod( t, read_identifier )
      Froley.parser_methods[ m.name ] = m
      parse_rule_type = ParseRuleType.GENERAL

      if (consume(TokenType.SYMBOL_COLON))
        t = peek
        local type_name = read_identifier
        which (type_name)
          case "ParseRule":          m.type = MethodTypeParseRule; parse_rule_type = ParseRuleType.GENERAL
          case "PreUnaryParseRule":  m.type = MethodTypePreUnaryParseRule; parse_rule_type = ParseRuleType.PRE_UNARY
          case "PostUnaryParseRule": m.type = MethodTypePostUnaryParseRule; parse_rule_type = ParseRuleType.POST_UNARY
          case "BinaryParseRule":    m.type = MethodTypeBinaryParseRule; parse_rule_type = ParseRuleType.BINARY
          case "RightAssociativeBinaryParseRule": m.type = MethodTypeRightAssociativeBinaryParseRule; parse_rule_type = ParseRuleType.BINARY
          others
            throw t.error( "No such rule type '$'."(type_name) )
        endWhich
      endIf
      must_consume( TokenType.EOL )

      this_method = m
      parse_multi_line_parser_statements( m.statements )

      return m

endClass
