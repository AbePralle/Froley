module Froley
  uses ParseKit<<Froley>>

$include "Assembler.rogue"
$include "Cmd.rogue"
$include "FroleyError.rogue"
$include "FroleyParser.rogue"
$include "FroleyTokenizer.rogue"
$include "Label.rogue"
$include "ParserAssembler.rogue"
$include "ParserMethod.rogue"
$include "ParserOpcode.rogue"
$include "ScanState.rogue"
$include "Token.rogue"
$include "TokenType.rogue"
$include "TokenizerAssembler.rogue"
$include "TokenizerOpcode.rogue"
$include "TokenizerVM.rogue"

Launcher( System.command_line_arguments )

class Launcher
  METHODS
    method init( args:String[] )
      local options = String[]
      local targets = String[]
      forEach (arg in args)
        if (arg.begins_with("--")) options.add( arg )
        else                       targets.add( arg )
      endForEach

      init( options, targets )

    method init( options:String[], targets:String[] )
      try
        forEach (option in options)
          which (option.before_first('='))
            others
              throw FroleyError( "Unrecognized option: " + option )
          endWhich
        endForEach

        forEach (target in targets)
          Froley = Froley() # new singleton
          Froley.parse( File(target) )
        endForEach

      catch (error:Error)
        Console.error.println error
        System.exit 1

      endTry
endClass

class TokenDef( name:String, type:Int32, symbol=null:String, attributes=0:Int32 )
  METHODS
    method to->String
      return "$=$ ($) [$]" (name,type,symbol,attributes)
endClass

class Froley [singleton]
  PROPERTIES
    token_defs_by_name    = LookupList<<TokenDef>>()
    token_defs_by_type    = Table<<Int32,TokenDef>>()
    token_defs_by_symbol  = LookupList<<TokenDef>>()
    last_token_type       = 0

    token_defs_by_section = StringTable<<TokenDef[]>>()
    token_attributes      = LookupList<<Int32>>()

    entry_points          = StringLookupList()

    tokenizer_code = StringBuilder( 2048 )
    parser_code    = StringBuilder( 2048 )

    parser_methods = LookupList<<ParserMethod>>()

  METHODS
    method parse( file:File )
      local source = file.load_as_string
      parse( file.filepath, source )

    method parse( filepath:String, source:String )
      collect_definitions_and_extract_code( filepath, source )

      local tokenizer_statements = CmdStatements()
      local parser = FroleyParser()
      parser.set_source( filepath, tokenizer_code->String )
      parser.consume_eols
      parser.parse_multi_line_tokenizer_statements( tokenizer_statements )
      tokenizer_statements.resolve

      forEach (def in token_defs_by_name)
        if (def.symbol) token_defs_by_symbol[ def.symbol ] = def
        else            token_defs_by_symbol[ def.name ]   = def
      endForEach

      local tokenizer_bytes = TokenizerAssembler().assemble( tokenizer_statements )

      parser.set_source( filepath, parser_code->String )
      parser.parse_parser_methods
      local parser_assembler = ParserAssembler()
      local parser_bytes = parser_assembler.assemble( parser_methods )

      #{
      local max_count  = 0
      local max_digits = 0
      forEach (def in token_defs_by_name)
        max_count = max_count.or_larger( def.name.count )
        max_digits = max_digits.or_larger( def.type.digit_count )
      endForEach
      }#

      local token_types = @[]
      forEach (def at index in token_defs_by_name)
        local symbol = select{ def.symbol || def.name }
        token_types.add( @{ name:def.name, type:def.type, symbol:symbol } )
      endForEach

      local tokenizer_opcodes = @[]
      forEach (name in TokenizerOpcode.names)
        tokenizer_opcodes.add( @{ :name, value:TokenizerOpcode(name)->Int32 } )
      endForEach

      local parser_cmd_type_names = @[]
      parser_cmd_type_names.add( forEach in parser_assembler.cmd_type_names )

      local parser_opcodes = @[]
      forEach (name in ParserOpcode.names)
        parser_opcodes.add( @{ :name, value:ParserOpcode(name)->Int32 } )
      endForEach

      #{
      local b64 = tokenizer_bytes.to_base64
      while (b64.count >= 64)
        println b64.leftmost(64)
        b64 = b64.rightmost(-64)
      endWhile
      if (b64.count) println b64
      }#
      #@trace tokenizer_bytes.count

      local language_name = filepath.before_last( ".froley", &ignore_case )
      local output =
      @{
        language: language_name,
        :token_types,
        :tokenizer_opcodes,
        tokenizer_code:tokenizer_bytes.to_base64,
        :parser_cmd_type_names,
        :parser_opcodes,
        parser_code:parser_bytes.to_base64
      }

      #@trace output
      local output_filepath = "$.json"(language_name)
      println "Writing " + output_filepath
      File.save( output_filepath, output.to_json )

      #local vm = TestVM( tokenizer_bytes )
      #vm.tokenize( File("Test.txt") )


    method collect_definitions_and_extract_code( filepath:String, source:String )
      $localDefine PARSING_CONFIGURE   0
      $localDefine PARSING_ATTRIBUTES  1
      $localDefine PARSING_DEFINITIONS 2
      $localDefine PARSING_TOKENIZER   3
      $localDefine PARSING_PARSER      4
      tokenizer_code.clear
      parser_code.clear
      local cur_section_name = "tokenizer"
      local cur_section_defs : TokenDef[]
      local parse_type = PARSING_TOKENIZER
      local buffer = StringBuilder()
      local next_attribute_value = 1
      forEach (line at index in LineReader(source))
        if (line.begins_with('['))  # new section
          tokenizer_code.println  # keep line numbers consistent
          parser_code.println  # keep line numbers consistent
          cur_section_name = line.extract_string( "[$]*" )
          which (cur_section_name)
            case "configure"
              parse_type = PARSING_CONFIGURE
            case "attributes"
              parse_type = PARSING_ATTRIBUTES
            case "tokenizer"
              parse_type = PARSING_TOKENIZER
            case "parser"
              parse_type = PARSING_PARSER
            others
              parse_type = PARSING_DEFINITIONS
          endWhich

          if (parse_type == PARSING_DEFINITIONS)
            cur_section_defs = section_defs( cur_section_name )
          endIf

        elseIf (parse_type == PARSING_TOKENIZER)
          tokenizer_code.println( line )
          parser_code.println  # keep line numbers consistent

        elseIf (parse_type == PARSING_PARSER)
          parser_code.println( line )
          tokenizer_code.println  # keep line numbers consistent

        elseIf (parse_type == PARSING_ATTRIBUTES)
          tokenizer_code.println  # keep line numbers consistent
          parser_code.println  # keep line numbers consistent
          local scanner = Scanner( line, &spaces_per_tab=2 ).[ line=index+1 ]
          discard_whitespace( scanner )
          if (scanner.has_another)
            local name = scanner.scan_identifier
            if (name is null)
              throw FroleyError( filepath, source, scanner.line, scanner.column, "Attribute name expected." )
            endIf

            discard_whitespace( scanner )
            if (scanner.consume('='))
              discard_whitespace( scanner )
              if (not scanner.peek.is_number)
                throw FroleyError( filepath, source, scanner.line, scanner.column, "Attribute value expected, e.g 1, 2, 4, etc." )
              endIf
              next_attribute_value = scanner.scan_int64
            endIf

            token_attributes[ name ] = next_attribute_value
            next_attribute_value *= 2

          endIf
        else
          # Token definitions
          tokenizer_code.println  # keep line numbers consistent
          parser_code.println  # keep line numbers consistent
          local scanner = Scanner( line, &spaces_per_tab=2 ).[ line=index+1 ]
          discard_whitespace( scanner )
          if (scanner.has_another)
            local name = scanner.scan_identifier
            if (name is null)
              throw FroleyError( filepath, source, scanner.line, scanner.column, "Token name expected." )
            endIf

            local symbol : String
            discard_whitespace( scanner )
            if (scanner.has_another)
              buffer.clear
              local ch = scanner.peek
              if (ch == '"' or ch == '\'')
                local st = scanner.scan_string
                if (st is null)
                  throw FroleyError( filepath, source, scanner.line, scanner.column, "Unterminated string." )
                endIf
                buffer.print( st )
              else
                while (scanner.has_another and not scanner.consume(' ')) buffer.print( scanner.read )
              endIf
              symbol = buffer->String
            endIf

            local attributes = 0
            discard_whitespace( scanner )
            if (scanner.consume('['))
              discard_whitespace( scanner )
              local first = true
              while (first or scanner.consume(','))
                first = false
                discard_whitespace( scanner )
                local attribute_name = scanner.scan_identifier
                if (attribute_name is null)
                  throw FroleyError( filepath, source, scanner.line, scanner.column, "Attribute name expected." )
                endIf
                if (not token_attributes.contains(attribute_name))
                  throw FroleyError( filepath, source, scanner.line, scanner.column, "Undefined attribute '$'." (attribute_name) )
                endIf
                attributes |= token_attributes[ attribute_name ]
              endWhile
              if (not scanner.consume(']'))
                throw FroleyError( filepath, source, scanner.line, scanner.column, "Closing ']' expected." )
              endIf
            endIf

            discard_whitespace( scanner )
            if (scanner.has_another)
              throw FroleyError( filepath, source, scanner.line, scanner.column, "Syntax error - unexpected '$'." (scanner.peek.to_escaped_ascii) )
            endIf

            local def = token_def( name, cur_section_name )
            def.symbol = symbol
            def.attributes = attributes

          endIf
        endIf
      endForEach

    method discard_whitespace( scanner:Scanner )
      while (scanner.consume(' ')) noAction
      if (scanner.consume('#'))
        while (scanner.has_another) scanner.read
      endIf

    method section_defs( name:String )->TokenDef[]
      local entry = token_defs_by_section.find( name )
      if (entry) return entry.value

      local defs = TokenDef[]
      token_defs_by_section[ name ] = defs
      return defs

    method token_def( token_name:String, cur_section_name=null:String )->TokenDef
      if (token_defs_by_name.contains(token_name)) return token_defs_by_name[ token_name ]

      ++last_token_type
      local def = TokenDef( token_name, last_token_type )
      token_defs_by_name[ token_name ] = def
      token_defs_by_type[ def.type ] = def

      if (cur_section_name)
        section_defs( cur_section_name ).add( def )
      endIf
      return def

    method token_def( token_type:Int32 )->TokenDef
      return token_defs_by_type[ token_type ]

endClass

class TestVM : TokenizerVM
  METHODS
    method accept( token_type:Int32 )
      @trace token_type, buffer
endClass

